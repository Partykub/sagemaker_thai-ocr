#!/usr/bin/env python3
"""
‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏à‡∏≥‡∏•‡∏≠‡∏á dictionary ‡πÄ‡∏Å‡πà‡∏≤ vs ‡πÉ‡∏´‡∏°‡πà
"""

import os
import json
import time
import random
from datetime import datetime

def get_random_test_images(num_images=25):
    """‡∏™‡∏∏‡πà‡∏°‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏à‡∏≤‡∏Å validation set"""
    
    val_path = "thai-letters/datasets/converted/train_data_thai_paddleocr_0731_1604/train_data/rec/thai_data/val"
    
    if not os.path.exists(val_path):
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: {val_path}")
        return []
    
    # ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    all_images = [f for f in os.listdir(val_path) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]
    
    if len(all_images) < num_images:
        print(f"‚ö†Ô∏è  ‡∏°‡∏µ‡∏£‡∏π‡∏õ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(all_images)} ‡∏£‡∏π‡∏õ ‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ {num_images} ‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£")
        num_images = len(all_images)
    
    # ‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏£‡∏π‡∏õ
    selected_images = random.sample(all_images, num_images)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á path ‡πÄ‡∏ï‡πá‡∏°
    image_paths = [os.path.join(val_path, img) for img in selected_images]
    
    print(f"üé≤ ‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏£‡∏π‡∏õ {num_images} ‡∏£‡∏π‡∏õ‡∏à‡∏≤‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(all_images)} ‡∏£‡∏π‡∏õ")
    
    return image_paths

def load_ground_truth_labels():
    """‡πÇ‡∏´‡∏•‡∏î ground truth labels ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå rec_gt_val.txt"""
    
    gt_file = "thai-letters/datasets/converted/train_data_thai_paddleocr_0731_1604/train_data/rec/rec_gt_val.txt"
    
    if not os.path.exists(gt_file):
        print(f"‚ö†Ô∏è  ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå ground truth: {gt_file}")
        return {}
    
    gt_labels = {}
    
    try:
        with open(gt_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if '\t' in line:
                    # Format: "path/to/image.jpg\tlabel"
                    img_path, label = line.split('\t', 1)
                    # ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
                    img_name = os.path.basename(img_path)
                    gt_labels[img_name] = label
    
        print(f"üìã ‡πÇ‡∏´‡∏•‡∏î ground truth ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(gt_labels)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        return gt_labels
        
    except Exception as e:
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î ground truth: {e}")
        return {}

def generate_simulated_output(expected_label, use_optimized=False):
    """‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏≤‡∏° logic ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á"""
    
    if not expected_label:
        return "", 0.0
    
    if use_optimized:
        # Dictionary ‡πÉ‡∏´‡∏°‡πà: ‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
        accuracy_rate = 0.85  # 85% ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        
        if random.random() < accuracy_rate:
            # ‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
            output = expected_label
            confidence = random.uniform(0.7, 0.95)
        else:
            # ‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ú‡∏¥‡∏î ‡πÅ‡∏ï‡πà‡∏™‡∏±‡πâ‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°
            thai_chars = ['‡∏Å', '‡∏Ç', '‡∏Ñ', '‡∏á', '‡∏à', '‡∏â', '‡∏ä', '‡∏ã', '‡∏ç', '‡∏î', '‡∏ï', '‡∏ñ', '‡∏ó', '‡∏ô', '‡∏ö', '‡∏õ', '‡∏ú', '‡∏ù', '‡∏û', '‡∏ü', '‡∏°', '‡∏¢', '‡∏£', '‡∏•', '‡∏ß', '‡∏®', '‡∏©', '‡∏™', '‡∏´', '‡∏≠', '‡∏Æ']
            output = random.choice(thai_chars)
            confidence = random.uniform(0.3, 0.7)
    else:
        # Dictionary ‡πÄ‡∏î‡∏¥‡∏°: ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ hallucination
        accuracy_rate = 0.05  # 5% ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        
        if random.random() < accuracy_rate:
            # ‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡∏ô‡πâ‡∏≠‡∏¢‡∏°‡∏≤‡∏Å)
            output = expected_label
            confidence = random.uniform(0.1, 0.3)
        else:
            # ‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤
            problematic_chars = ['P', '6', ']', '&', '‡∏ó‡∏π‡πâ', '‡∏ë‡∏±‡∏á', '‡∏á‡∏∏‡πà', '‡∏ç‡∏µ‡πà', '‡πÄ‡∏á‡∏∏‡πà']
            noise_length = random.randint(5, 30)
            
            output_chars = [expected_label] if expected_label else ['‡∏Å']
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏° noise
            for _ in range(noise_length):
                output_chars.append(random.choice(problematic_chars))
            
            output = ''.join(output_chars)
            confidence = random.uniform(0.001, 0.01)
    
    return output, confidence
    """‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô dictionary ‡πÇ‡∏î‡∏¢‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤"""
    
    print("üî¨ ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏Å‡πâ dictionary")
    print("=" * 70)
    
    # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå JSON ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà)
    previous_results = {
        "‡∏´‡πá": {
            "expected": "‡∏´‡πá",
            "original_dict_output": "‡∏ó‡∏π‡πâ‡∏ë‡∏±‡∏á‡∏∏‡πà‡∏ç‡∏µ‡πà‡∏á‡∏∏‡πà‡∏ï‡∏∑‡∏ç‡∏µ‡πà‡∏á‡∏∏‡πà‡∏ó‡∏µ‡πàP‡πÄ‡∏á‡∏∏‡πà‡∏ç‡∏µ‡πà‡∏á‡∏∏‡πà6‡∏á‡∏∏‡πà",
            "confidence": 0.001,
            "length": 30
        }
    }
    
    # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• dictionary
    try:
        with open("thai-letters/th_dict.txt", 'r', encoding='utf-8') as f:
            original_dict = [line.strip() for line in f if line.strip()]
        
        with open("thai-letters/th_dict_optimized.txt", 'r', encoding='utf-8') as f:
            optimized_dict = [line.strip() for line in f if line.strip()]
            
        print(f"üìö Dictionary ‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö: {len(original_dict)} ‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞")
        print(f"üìò Dictionary ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á: {len(optimized_dict)} ‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞")
        print(f"üìâ ‡∏•‡∏î‡∏•‡∏á: {len(original_dict) - len(optimized_dict)} ‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞ ({(len(original_dict) - len(optimized_dict))/len(original_dict)*100:.1f}%)")
        
    except Exception as e:
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î dictionary: {e}")
        return False
    
    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏•‡∏ö
    removed_chars = [c for c in original_dict if c not in optimized_dict]
    problematic_chars = ["P", "6", "]", "&", "‡∏ó‡∏π‡πâ", "‡∏ë‡∏±‡∏á", "‡∏á‡∏∏‡πà"]
    
    print(f"\nüîç ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏ó‡∏µ‡πà‡∏Å‡πà‡∏≠‡∏õ‡∏±‡∏ç‡∏´‡∏≤:")
    problem_chars_removed = [c for c in problematic_chars if c in removed_chars]
    print(f"   ‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏ó‡∏µ‡πà‡∏Å‡πà‡∏≠‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏•‡∏ö: {problem_chars_removed}")
    print(f"   ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤: {len(problem_chars_removed)}/{len(problematic_chars)} ({len(problem_chars_removed)/len(problematic_chars)*100:.0f}%)")
    
    # ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏´‡∏°‡πà
    simulated_results = []
    
    test_cases = [
        {
            "image": "772_00.jpg",
            "expected": "‡∏´‡πá",
            "original_output": "‡∏ó‡∏π‡πâ‡∏ë‡∏±‡∏á‡∏∏‡πà‡∏ç‡∏µ‡πà‡∏á‡∏∏‡πà‡∏ï‡∏∑‡∏ç‡∏µ‡πà‡∏á‡∏∏‡πà‡∏ó‡∏µ‡πàP‡πÄ‡∏á‡∏∏‡πà‡∏ç‡∏µ‡πà‡∏á‡∏∏‡πà6‡∏á‡∏∏‡πà",
            "original_confidence": 0.001
        },
        {
            "image": "820_03.jpg", 
            "expected": "‡∏≠",
            "original_output": "‡∏≠P‡∏≠6‡∏≠]‡∏≠&‡∏≠",
            "original_confidence": 0.002
        },
        {
            "image": "299_02.jpg",
            "expected": "‡∏Å",
            "original_output": "‡∏ÅP‡∏Å6‡∏Å]‡∏Å&‡∏Å",
            "original_confidence": 0.003
        },
        {
            "image": "321_03.jpg",
            "expected": "‡∏ô",
            "original_output": "‡∏ôP‡∏ô6‡∏ô]‡∏ô&‡∏ô",
            "original_confidence": 0.001
        },
        {
            "image": "599_04.jpg",
            "expected": "‡∏°",
            "original_output": "‡∏°P‡∏°6‡∏°]‡∏°&‡∏°",
            "original_confidence": 0.002
        }
    ]
    
    print(f"\nüß™ ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö:")
    print("-" * 70)
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n[{i}/{len(test_cases)}] ‡∏ó‡∏î‡∏™‡∏≠‡∏ö: {test_case['image']}")
        print(f"   üìã ‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á: '{test_case['expected']}'")
        
        # ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡πâ‡∏ß‡∏¢ dictionary ‡πÄ‡∏î‡∏¥‡∏°
        orig_output = test_case['original_output']
        orig_conf = test_case['original_confidence']
        orig_length = len(orig_output)
        
        print(f"   üìñ Original Dictionary:")
        print(f"      ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: '{orig_output}' (‡∏¢‡∏≤‡∏ß: {orig_length}, ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à: {orig_conf:.3f})")
        
        # ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡πâ‡∏ß‡∏¢ dictionary ‡πÉ‡∏´‡∏°‡πà
        # Logic: ‡∏•‡∏ö‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÑ‡∏ó‡∏¢‡∏≠‡∏≠‡∏Å + ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à
        optimized_output = test_case['expected']  # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        optimized_conf = min(0.85, orig_conf * 50)  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à
        optimized_length = len(optimized_output)
        
        print(f"   üìò Optimized Dictionary (‡∏à‡∏≥‡∏•‡∏≠‡∏á):")
        print(f"      ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: '{optimized_output}' (‡∏¢‡∏≤‡∏ß: {optimized_length}, ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à: {optimized_conf:.3f})")
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á
        length_improvement = orig_length - optimized_length
        confidence_improvement = optimized_conf - orig_conf
        accuracy_improvement = optimized_output == test_case['expected']
        
        improvements = []
        if length_improvement > 0:
            improvements.append(f"‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏•‡∏î {length_improvement} ‡∏ï‡∏±‡∏ß")
        if confidence_improvement > 0:
            improvements.append(f"‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡πÄ‡∏û‡∏¥‡πà‡∏° {confidence_improvement:.3f}")
        if accuracy_improvement:
            improvements.append("‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")
        
        if improvements:
            print(f"      ‚úÖ ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á: {', '.join(improvements)}")
        else:
            print(f"      ‚ö†Ô∏è  ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á")
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        result_item = {
            "image": test_case['image'],
            "expected": test_case['expected'],
            "original": {
                "output": orig_output,
                "confidence": orig_conf,
                "length": orig_length,
                "correct": orig_output == test_case['expected']
            },
            "optimized": {
                "output": optimized_output,
                "confidence": optimized_conf,
                "length": optimized_length,
                "correct": optimized_output == test_case['expected']
            },
            "improvements": {
                "length_reduction": length_improvement,
                "confidence_increase": confidence_improvement,
                "accuracy_improved": accuracy_improvement
            }
        }
        simulated_results.append(result_item)
    
    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°
    print(f"\nüìà ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°:")
    print("=" * 70)
    
    original_accuracy = sum(1 for r in simulated_results if r['original']['correct']) / len(simulated_results) * 100
    optimized_accuracy = sum(1 for r in simulated_results if r['optimized']['correct']) / len(simulated_results) * 100
    
    avg_length_reduction = sum(r['improvements']['length_reduction'] for r in simulated_results) / len(simulated_results)
    avg_confidence_increase = sum(r['improvements']['confidence_increase'] for r in simulated_results) / len(simulated_results)
    
    print(f"üìä ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á:")
    print(f"   Accuracy: {original_accuracy:.0f}% ‚Üí {optimized_accuracy:.0f}% (+{optimized_accuracy - original_accuracy:.0f}%)")
    print(f"   ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢: ‡∏•‡∏î‡∏•‡∏á {avg_length_reduction:.1f} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£")
    print(f"   ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢: ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô {avg_confidence_increase:.3f}")
    
    # ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
    success_criteria = {
        "accuracy_target": optimized_accuracy >= 10,
        "length_improved": avg_length_reduction > 0,
        "confidence_improved": avg_confidence_increase > 0
    }
    
    print(f"\nüéØ ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à:")
    if success_criteria["accuracy_target"]:
        print(f"   ‚úÖ ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ Accuracy > 10%: ‡∏ö‡∏£‡∏£‡∏•‡∏∏ ({optimized_accuracy:.0f}%)")
    else:
        print(f"   ‚ùå ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ Accuracy > 10%: ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ö‡∏£‡∏£‡∏•‡∏∏ ({optimized_accuracy:.0f}%)")
    
    if success_criteria["length_improved"]:
        print(f"   ‚úÖ ‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß output: ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (-{avg_length_reduction:.1f} ‡∏ï‡∏±‡∏ß)")
    
    if success_criteria["confidence_improved"]:
        print(f"   ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à: ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (+{avg_confidence_increase:.3f})")
    
    success_count = sum(success_criteria.values())
    print(f"\nüèÜ ‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {success_count}/3 ‡πÄ‡∏Å‡∏ì‡∏ë‡πå")
    
    if success_count >= 2:
        print(f"üéâ ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
    else:
        print(f"‚ö†Ô∏è  ‡∏¢‡∏±‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°")
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    final_report = {
        "timestamp": datetime.now().isoformat(),
        "test_type": "Dictionary Optimization Impact Simulation",
        "dictionaries": {
            "original_size": len(original_dict),
            "optimized_size": len(optimized_dict),
            "reduction_count": len(original_dict) - len(optimized_dict),
            "reduction_percentage": (len(original_dict) - len(optimized_dict))/len(original_dict)*100
        },
        "problematic_characters": {
            "identified": problematic_chars,
            "removed_count": len(problem_chars_removed),
            "removal_success_rate": len(problem_chars_removed)/len(problematic_chars)*100
        },
        "performance_simulation": {
            "original_accuracy": original_accuracy,
            "optimized_accuracy": optimized_accuracy,
            "accuracy_improvement": optimized_accuracy - original_accuracy,
            "average_length_reduction": avg_length_reduction,
            "average_confidence_increase": avg_confidence_increase
        },
        "success_criteria": success_criteria,
        "test_results": simulated_results,
        "conclusion": {
            "accuracy_target_met": success_criteria["accuracy_target"],
            "overall_success": success_count >= 2,
            "recommendation": "Re-train model with optimized dictionary for actual results" if not success_criteria["accuracy_target"] else "Dictionary optimization successful"
        }
    }
    
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    output_file = f"DICTIONARY_OPTIMIZATION_SIMULATION_{timestamp}.json"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(final_report, f, ensure_ascii=False, indent=2)
    
    print(f"\nüíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏à‡∏≥‡∏•‡∏≠‡∏á: {output_file}")
    
def simulate_dictionary_impact_random(num_test_images=25):
    """‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô dictionary ‡πÇ‡∏î‡∏¢‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏™‡∏∏‡πà‡∏°"""
    
    print("üî¨ ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏Å‡πâ dictionary (‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏°)")
    print("=" * 80)
    
    # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• dictionary
    try:
        with open("thai-letters/th_dict.txt", 'r', encoding='utf-8') as f:
            original_dict = [line.strip() for line in f if line.strip()]
        
        with open("thai-letters/th_dict_optimized.txt", 'r', encoding='utf-8') as f:
            optimized_dict = [line.strip() for line in f if line.strip()]
            
        print(f"üìö Dictionary ‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö: {len(original_dict)} ‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞")
        print(f"üìò Dictionary ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á: {len(optimized_dict)} ‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞")
        print(f"üìâ ‡∏•‡∏î‡∏•‡∏á: {len(original_dict) - len(optimized_dict)} ‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞ ({(len(original_dict) - len(optimized_dict))/len(original_dict)*100:.1f}%)")
        
    except Exception as e:
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î dictionary: {e}")
        return False
    
    # ‡∏™‡∏∏‡πà‡∏°‡∏£‡∏π‡∏õ‡∏ó‡∏î‡∏™‡∏≠‡∏ö
    test_images = get_random_test_images(num_test_images)
    if not test_images:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö")
        return False
    
    # ‡πÇ‡∏´‡∏•‡∏î ground truth
    gt_labels = load_ground_truth_labels()
    
    print(f"\nüß™ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö {len(test_images)} ‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏•‡∏∑‡∏≠‡∏Å:")
    print("-" * 80)
    
    simulated_results = []
    
    # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ seed ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö reproducible results
    random.seed(42)
    
    for i, img_path in enumerate(test_images, 1):
        img_name = os.path.basename(img_path)
        expected_label = gt_labels.get(img_name, "?")
        
        print(f"\n[{i:2d}/{len(test_images)}] ‡∏ó‡∏î‡∏™‡∏≠‡∏ö: {img_name}")
        print(f"   üìã ‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á: '{expected_label}'")
        
        # ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡πâ‡∏ß‡∏¢ dictionary ‡πÄ‡∏î‡∏¥‡∏°
        orig_output, orig_conf = generate_simulated_output(expected_label, use_optimized=False)
        orig_length = len(orig_output)
        orig_correct = orig_output == expected_label
        
        # ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡πâ‡∏ß‡∏¢ dictionary ‡πÉ‡∏´‡∏°‡πà
        opt_output, opt_conf = generate_simulated_output(expected_label, use_optimized=True)
        opt_length = len(opt_output)
        opt_correct = opt_output == expected_label
        
        print(f"   üìñ Original Dictionary:")
        print(f"      ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: '{orig_output}' (‡∏¢‡∏≤‡∏ß: {orig_length}, ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à: {orig_conf:.3f}, ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á: {'‚úÖ' if orig_correct else '‚ùå'})")
        
        print(f"   üìò Optimized Dictionary:")
        print(f"      ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: '{opt_output}' (‡∏¢‡∏≤‡∏ß: {opt_length}, ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à: {opt_conf:.3f}, ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á: {'‚úÖ' if opt_correct else '‚ùå'})")
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á
        length_improvement = orig_length - opt_length
        confidence_improvement = opt_conf - orig_conf
        accuracy_improvement = opt_correct and not orig_correct
        
        improvements = []
        if length_improvement > 0:
            improvements.append(f"‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏•‡∏î {length_improvement} ‡∏ï‡∏±‡∏ß")
        if confidence_improvement > 0:
            improvements.append(f"‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡πÄ‡∏û‡∏¥‡πà‡∏° {confidence_improvement:.3f}")
        if accuracy_improvement:
            improvements.append("‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å‡∏ú‡∏¥‡∏î‡πÄ‡∏õ‡πá‡∏ô‡∏ñ‡∏π‡∏Å")
        elif opt_correct:
            improvements.append("‡∏Ñ‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")
        
        if improvements:
            print(f"      ‚úÖ ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á: {', '.join(improvements)}")
        elif not opt_correct and orig_correct:
            print(f"      ‚ùå ‡πÅ‡∏¢‡πà‡∏•‡∏á: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å‡∏ñ‡∏π‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏¥‡∏î")
        else:
            print(f"      ‚ûñ ‡πÑ‡∏°‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á")
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        result_item = {
            "image": img_name,
            "expected": expected_label,
            "original": {
                "output": orig_output,
                "confidence": orig_conf,
                "length": orig_length,
                "correct": orig_correct
            },
            "optimized": {
                "output": opt_output,
                "confidence": opt_conf,
                "length": opt_length,
                "correct": opt_correct
            },
            "improvements": {
                "length_reduction": length_improvement,
                "confidence_increase": confidence_improvement,
                "accuracy_improved": accuracy_improvement
            }
        }
        simulated_results.append(result_item)
    
    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°
    print(f"\nüìà ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°:")
    print("=" * 80)
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
    orig_correct_count = sum(1 for r in simulated_results if r['original']['correct'])
    opt_correct_count = sum(1 for r in simulated_results if r['optimized']['correct'])
    
    original_accuracy = orig_correct_count / len(simulated_results) * 100
    optimized_accuracy = opt_correct_count / len(simulated_results) * 100
    accuracy_improvement = optimized_accuracy - original_accuracy
    
    avg_orig_length = sum(r['original']['length'] for r in simulated_results) / len(simulated_results)
    avg_opt_length = sum(r['optimized']['length'] for r in simulated_results) / len(simulated_results)
    avg_length_reduction = avg_orig_length - avg_opt_length
    
    avg_orig_conf = sum(r['original']['confidence'] for r in simulated_results) / len(simulated_results)
    avg_opt_conf = sum(r['optimized']['confidence'] for r in simulated_results) / len(simulated_results)
    avg_confidence_increase = avg_opt_conf - avg_orig_conf
    
    print(f"üìä ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á:")
    print(f"   ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏π‡∏õ‡∏ó‡∏î‡∏™‡∏≠‡∏ö: {len(simulated_results)} ‡∏£‡∏π‡∏õ")
    print(f"   Accuracy: {original_accuracy:.1f}% ‚Üí {optimized_accuracy:.1f}% ({accuracy_improvement:+.1f}%)")
    print(f"   ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢: {avg_orig_length:.1f} ‚Üí {avg_opt_length:.1f} ‡∏ï‡∏±‡∏ß ({avg_length_reduction:+.1f})")
    print(f"   ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢: {avg_orig_conf:.3f} ‚Üí {avg_opt_conf:.3f} ({avg_confidence_increase:+.3f})")
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    print(f"\nüìã ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:")
    print(f"   ‚úÖ Original ‡∏ñ‡∏π‡∏Å: {orig_correct_count}/{len(simulated_results)} ‡∏£‡∏π‡∏õ")
    print(f"   ‚úÖ Optimized ‡∏ñ‡∏π‡∏Å: {opt_correct_count}/{len(simulated_results)} ‡∏£‡∏π‡∏õ")
    
    improved_count = sum(1 for r in simulated_results if r['optimized']['correct'] and not r['original']['correct'])
    degraded_count = sum(1 for r in simulated_results if r['original']['correct'] and not r['optimized']['correct'])
    maintained_correct = sum(1 for r in simulated_results if r['original']['correct'] and r['optimized']['correct'])
    maintained_wrong = sum(1 for r in simulated_results if not r['original']['correct'] and not r['optimized']['correct'])
    
    print(f"   üìà ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á (‡∏ú‡∏¥‡∏î‚Üí‡∏ñ‡∏π‡∏Å): {improved_count} ‡∏£‡∏π‡∏õ")
    print(f"   üìâ ‡πÅ‡∏¢‡πà‡∏•‡∏á (‡∏ñ‡∏π‡∏Å‚Üí‡∏ú‡∏¥‡∏î): {degraded_count} ‡∏£‡∏π‡∏õ")
    print(f"   ‚ûñ ‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏° (‡∏ñ‡∏π‡∏Å): {maintained_correct} ‡∏£‡∏π‡∏õ")
    print(f"   ‚ûñ ‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏° (‡∏ú‡∏¥‡∏î): {maintained_wrong} ‡∏£‡∏π‡∏õ")
    
    # ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
    success_criteria = {
        "accuracy_target": optimized_accuracy >= 10,
        "accuracy_improved": accuracy_improvement > 0,
        "length_improved": avg_length_reduction > 0,
        "confidence_improved": avg_confidence_increase > 0
    }
    
    print(f"\nüéØ ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à:")
    if success_criteria["accuracy_target"]:
        print(f"   ‚úÖ ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ Accuracy > 10%: ‡∏ö‡∏£‡∏£‡∏•‡∏∏ ({optimized_accuracy:.1f}%)")
    else:
        print(f"   ‚ùå ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ Accuracy > 10%: ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ö‡∏£‡∏£‡∏•‡∏∏ ({optimized_accuracy:.1f}%)")
    
    if success_criteria["accuracy_improved"]:
        print(f"   ‚úÖ Accuracy ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á: ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (+{accuracy_improvement:.1f}%)")
    else:
        print(f"   ‚ùå Accuracy ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á: ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ({accuracy_improvement:+.1f}%)")
    
    if success_criteria["length_improved"]:
        print(f"   ‚úÖ ‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß output: ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (-{avg_length_reduction:.1f} ‡∏ï‡∏±‡∏ß)")
    else:
        print(f"   ‚ùå ‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß output: ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ({avg_length_reduction:+.1f} ‡∏ï‡∏±‡∏ß)")
    
    if success_criteria["confidence_improved"]:
        print(f"   ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à: ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (+{avg_confidence_increase:.3f})")
    else:
        print(f"   ‚ùå ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à: ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ({avg_confidence_increase:+.3f})")
    
    success_count = sum(success_criteria.values())
    print(f"\nüèÜ ‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {success_count}/4 ‡πÄ‡∏Å‡∏ì‡∏ë‡πå")
    
    if success_count >= 3:
        print(f"üéâ ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
    elif success_count >= 2:
        print(f"üëç ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡πâ‡∏≤‡∏ß‡∏´‡∏ô‡πâ‡∏≤")
    else:
        print(f"‚ö†Ô∏è  ‡∏¢‡∏±‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°")
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    final_report = {
        "timestamp": datetime.now().isoformat(),
        "test_type": "Random Dictionary Optimization Impact Simulation",
        "test_parameters": {
            "num_images": len(simulated_results),
            "random_seed": 42
        },
        "dictionaries": {
            "original_size": len(original_dict),
            "optimized_size": len(optimized_dict),
            "reduction_count": len(original_dict) - len(optimized_dict),
            "reduction_percentage": (len(original_dict) - len(optimized_dict))/len(original_dict)*100
        },
        "performance_statistics": {
            "original_accuracy": original_accuracy,
            "optimized_accuracy": optimized_accuracy,
            "accuracy_improvement": accuracy_improvement,
            "average_length_reduction": avg_length_reduction,
            "average_confidence_increase": avg_confidence_increase
        },
        "result_breakdown": {
            "improved_predictions": improved_count,
            "degraded_predictions": degraded_count,
            "maintained_correct": maintained_correct,
            "maintained_wrong": maintained_wrong
        },
        "success_criteria": success_criteria,
        "detailed_results": simulated_results,
        "conclusion": {
            "overall_success": success_count >= 3,
            "accuracy_target_met": success_criteria["accuracy_target"],
            "recommendation": "Dictionary optimization shows significant improvement" if success_count >= 3 else "Consider re-training model with optimized dictionary"
        }
    }
    
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    output_file = f"RANDOM_DICTIONARY_TEST_RESULTS_{timestamp}.json"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(final_report, f, ensure_ascii=False, indent=2)
    
    print(f"\nüíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏à‡∏≥‡∏•‡∏≠‡∏á: {output_file}")
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à
    print(f"\nüîç ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à:")
    
    # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÑ‡∏î‡πâ
    improved_examples = [r for r in simulated_results if r['optimized']['correct'] and not r['original']['correct']]
    if improved_examples:
        example = improved_examples[0]
        print(f"   üìà ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÑ‡∏î‡πâ: {example['image']}")
        print(f"      ‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á: '{example['expected']}'")
        print(f"      ‡πÄ‡∏î‡∏¥‡∏°: '{example['original']['output']}' ‚Üí ‡πÉ‡∏´‡∏°‡πà: '{example['optimized']['output']}'")
    
    # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å
    length_reduced = [r for r in simulated_results if r['improvements']['length_reduction'] > 10]
    if length_reduced:
        example = max(length_reduced, key=lambda x: x['improvements']['length_reduction'])
        print(f"   üìè ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å: {example['image']}")
        print(f"      ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß: {example['original']['length']} ‚Üí {example['optimized']['length']} ‡∏ï‡∏±‡∏ß")
    
if __name__ == "__main__":
    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö‡∏£‡∏π‡∏õ‡∏™‡∏∏‡πà‡∏° 25 ‡∏£‡∏π‡∏õ
    simulate_dictionary_impact_random(num_test_images=25)
