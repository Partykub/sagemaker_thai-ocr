#!/usr/bin/env python3
"""
Direct Paddle Inference: ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡∏°‡∏≤‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏ú‡πà‡∏≤‡∏ô Paddle inference API
"""

import os
import sys
import cv2
import numpy as np
from pathlib import Path
import paddle
from typing import Tuple, List

# Add PaddleOCR path for utilities
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root / "PaddleOCR"))

class DirectPaddleInference:
    """‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡∏°‡∏≤‡∏ú‡πà‡∏≤‡∏ô Paddle inference ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á"""
    
    def __init__(self, model_dir: Path):
        self.model_dir = model_dir
        self.model_path = model_dir / "best_model" / "model"  # ‡πÉ‡∏ä‡πâ best_model/model.pdparams
        self.predictor = None
        self.char_dict = None
        
    def load_character_dict(self) -> bool:
        """‡πÇ‡∏´‡∏•‡∏î character dictionary"""
        # ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå dict
        dict_files = [
            self.model_dir.parent.parent / "th_dict.txt",
            self.model_dir.parent.parent / "PaddleOCR" / "ppocr" / "utils" / "dict" / "th_dict.txt",
            self.model_dir.parent.parent / "thai-letters" / "th_dict.txt"
        ]
        
        for dict_file in dict_files:
            if dict_file.exists():
                try:
                    with open(dict_file, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                        self.char_dict = ['blank'] + [line.strip() for line in lines if line.strip()]
                    print(f"‚úÖ Loaded character dictionary: {len(self.char_dict)} characters from {dict_file.name}")
                    return True
                except Exception as e:
                    print(f"‚ö†Ô∏è Error loading {dict_file}: {e}")
                    continue
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á basic dict ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠
        self._create_basic_thai_dict()
        return True
    
    def _create_basic_thai_dict(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á character dictionary ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô"""
        thai_chars = [
            # ‡∏û‡∏¢‡∏±‡∏ç‡∏ä‡∏ô‡∏∞
            '‡∏Å', '‡∏Ç', '‡∏É', '‡∏Ñ', '‡∏Ö', '‡∏Ü', '‡∏á', '‡∏à', '‡∏â', '‡∏ä', '‡∏ã', '‡∏å', '‡∏ç',
            '‡∏é', '‡∏è', '‡∏ê', '‡∏ë', '‡∏í', '‡∏ì', '‡∏î', '‡∏ï', '‡∏ñ', '‡∏ó', '‡∏ò', '‡∏ô', '‡∏ö',
            '‡∏õ', '‡∏ú', '‡∏ù', '‡∏û', '‡∏ü', '‡∏†', '‡∏°', '‡∏¢', '‡∏£', '‡∏•', '‡∏ß', '‡∏®', '‡∏©',
            '‡∏™', '‡∏´', '‡∏¨', '‡∏≠', '‡∏Æ',
            # ‡∏™‡∏£‡∏∞‡πÅ‡∏•‡∏∞‡∏ß‡∏£‡∏£‡∏ì‡∏¢‡∏∏‡∏Å‡∏ï‡πå
            '‡∏∞', '‡∏±', '‡∏≤', '‡∏≥', '‡∏¥', '‡∏µ', '‡∏∂', '‡∏∑', '‡∏∏', '‡∏π', '‡πÄ', '‡πÅ', '‡πÇ',
            '‡πÉ', '‡πÑ', '‡πÖ', '‡πÜ', '‡πá', '‡πà', '‡πâ', '‡πä', '‡πã', '‡πå',
            # ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
            '‡πê', '‡πë', '‡πí', '‡πì', '‡πî', '‡πï', '‡πñ', '‡πó', '‡πò', '‡πô',
            # ‡∏û‡∏¥‡πÄ‡∏®‡∏©
            '‡∏Ø', '‡πè', '‡πö', '‡πõ', ' '
        ]
        
        self.char_dict = ['blank'] + thai_chars
        print(f"‚úÖ Created basic Thai dictionary: {len(self.char_dict)} characters")
    
    def initialize_inference(self) -> bool:
        """‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Paddle inference"""
        try:
            # ‡πÉ‡∏ä‡πâ best_model/model.pdparams ‡πÅ‡∏•‡∏∞ model.pdmodel
            model_file = str(self.model_path) + ".pdmodel"
            param_file = str(self.model_path) + ".pdparams"
            
            print(f"üîç Looking for model files:")
            print(f"   Model: {model_file}")
            print(f"   Params: {param_file}")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå
            if not Path(model_file).exists():
                print(f"‚ùå Model file not found: {model_file}")
                return False
            
            if not Path(param_file).exists():
                print(f"‚ùå Params file not found: {param_file}")
            
            # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ inference config
            config = paddle.inference.Config()
            config.set_model(model_file, param_file)
            print(f"‚úÖ Using model files: {Path(model_file).name}, {Path(param_file).name}")
            
            # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
            config.disable_gpu()  # ‡πÉ‡∏ä‡πâ CPU
            config.set_cpu_math_library_num_threads(4)
            config.switch_use_feed_fetch_ops(False)
            config.switch_specify_input_names(True)
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á predictor
            self.predictor = paddle.inference.create_predictor(config)
            print("‚úÖ Paddle inference initialized successfully")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Failed to initialize inference: {e}")
            return False
            
            print("‚úÖ Paddle inference initialized successfully")
            return True
            
        except Exception as e:
            print(f"‚ùå Failed to initialize Paddle inference: {e}")
            return False
    
    def preprocess_image(self, img: np.ndarray) -> np.ndarray:
        """‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö inference"""
        try:
            # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô grayscale ‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
            if len(img.shape) == 3:
                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            
            # Resize ‡∏ï‡∏≤‡∏°‡∏Ç‡∏ô‡∏≤‡∏î‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á (32x128 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö recognition)
            target_height = 32
            target_width = 128
            
            img_resized = cv2.resize(img, (target_width, target_height))
            
            # Normalize ‡πÄ‡∏õ‡πá‡∏ô [0, 1]
            img_normalized = img_resized.astype(np.float32) / 255.0
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏° channel dimension ‡πÅ‡∏•‡∏∞ batch dimension
            img_final = np.expand_dims(img_normalized, axis=0)  # (1, 32, 128)
            img_final = np.expand_dims(img_final, axis=0)      # (1, 1, 32, 128)
            
            return img_final
            
        except Exception as e:
            print(f"‚ùå Error preprocessing image: {e}")
            return None
    
    def run_inference(self, img: np.ndarray) -> Tuple[str, float]:
        """‡∏£‡∏±‡∏ô inference ‡∏Å‡∏±‡∏ö‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û"""
        try:
            if self.predictor is None:
                return "", 0.0
            
            # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏£‡∏π‡∏õ
            processed_img = self.preprocess_image(img)
            if processed_img is None:
                return "", 0.0
            
            # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ input
            input_names = self.predictor.get_input_names()
            input_tensor = self.predictor.get_input_handle(input_names[0])
            input_tensor.reshape(processed_img.shape)
            input_tensor.copy_from_cpu(processed_img)
            
            # ‡∏£‡∏±‡∏ô inference
            self.predictor.run()
            
            # ‡∏î‡∏∂‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            output_names = self.predictor.get_output_names()
            output_tensor = self.predictor.get_output_handle(output_names[0])
            output_data = output_tensor.copy_to_cpu()
            
            # ‡πÅ‡∏õ‡∏•‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
            text, confidence = self._decode_output(output_data)
            
            return text, confidence
            
        except Exception as e:
            print(f"‚ùå Inference error: {e}")
            return "", 0.0
    
    def _decode_output(self, output_data: np.ndarray) -> Tuple[str, float]:
        """‡πÅ‡∏õ‡∏•‡∏á output ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
        try:
            # CTC decoding
            if len(output_data.shape) == 3:
                output_data = output_data[0]  # remove batch dimension
            
            # Argmax ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ character indices
            indices = np.argmax(output_data, axis=1)
            
            # CTC decoding - remove blanks and duplicates
            decoded_chars = []
            prev_char = -1
            
            for idx in indices:
                if idx != 0 and idx != prev_char:  # 0 is blank
                    if idx < len(self.char_dict):
                        decoded_chars.append(self.char_dict[idx])
                prev_char = idx
            
            text = ''.join(decoded_chars)
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì confidence (average of max probabilities)
            max_probs = np.max(output_data, axis=1)
            confidence = np.mean(max_probs)
            
            return text, float(confidence)
            
        except Exception as e:
            print(f"‚ùå Decoding error: {e}")
            return "", 0.0

def test_direct_inference():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö direct inference"""
    print("üß™ Testing Direct Paddle Inference")
    print("=" * 50)
    
    project_root = Path(__file__).parent.parent.parent
    model_dir = project_root / "models" / "sagemaker_trained"
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á inference engine
    inference = DirectPaddleInference(model_dir)
    
    # ‡πÇ‡∏´‡∏•‡∏î character dictionary
    if not inference.load_character_dict():
        print("‚ùå Failed to load character dictionary")
        return False
    
    # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô inference
    if not inference.initialize_inference():
        print("‚ùå Failed to initialize inference")
        return False
    
    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö‡∏£‡∏π‡∏õ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
    test_img_dir = project_root / "thai-letters" / "datasets" / "converted" / "train_data_thai_paddleocr_0731_1604" / "train_data" / "rec" / "thai_data" / "val"
    
    if test_img_dir.exists():
        test_images = list(test_img_dir.glob("*.jpg"))[:5]  # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö 5 ‡∏£‡∏π‡∏õ‡πÅ‡∏£‡∏Å
        
        print(f"\nüéØ Testing with {len(test_images)} sample images:")
        
        for i, img_path in enumerate(test_images, 1):
            print(f"\nüì∑ Test {i}: {img_path.name}")
            
            # ‡∏≠‡πà‡∏≤‡∏ô‡∏£‡∏π‡∏õ
            img = cv2.imread(str(img_path))
            if img is None:
                print("‚ùå Cannot read image")
                continue
            
            # ‡∏£‡∏±‡∏ô inference
            text, confidence = inference.run_inference(img)
            print(f"   Result: '{text}' (confidence: {confidence:.4f})")
    
    print("\n‚úÖ Direct inference test completed!")
    return True

if __name__ == "__main__":
    test_direct_inference()
