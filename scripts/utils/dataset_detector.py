#!/usr/bin/env python3
"""
Dataset Detection Helper: à¸«à¸²à¹„à¸Ÿà¸¥à¹Œ dataset à¹à¸¥à¸° validation data à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£à¸—à¸”à¸ªà¸­à¸š
"""

import os
from pathlib import Path
from typing import List, Dict, Optional

def find_datasets() -> Dict[str, List[Path]]:
    """à¸«à¸² dataset à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¹ƒà¸™à¹‚à¸›à¸£à¹€à¸ˆà¸„"""
    
    project_root = Path(__file__).parent.parent.parent
    datasets = {
        "converted": [],
        "raw": [],
        "output": []
    }
    
    print("ğŸ” Searching for datasets...")
    
    # à¸«à¸²à¹ƒà¸™ thai-letters/datasets
    thai_letters_datasets = project_root / "thai-letters" / "datasets"
    if thai_letters_datasets.exists():
        print(f"ğŸ“ Checking: {thai_letters_datasets}")
        
        for item in thai_letters_datasets.iterdir():
            if item.is_dir():
                if "converted" in item.name or "paddleocr" in item.name:
                    datasets["converted"].append(item)
                    print(f"   âœ… Found converted dataset: {item.name}")
                else:
                    datasets["raw"].append(item)
                    print(f"   ğŸ“Š Found raw dataset: {item.name}")
    
    # à¸«à¸²à¹ƒà¸™ train_data_thai_paddleocr_* folders
    for item in project_root.iterdir():
        if item.is_dir() and item.name.startswith("train_data_thai_paddleocr"):
            datasets["converted"].append(item)
            print(f"   âœ… Found converted dataset: {item.name}")
    
    # à¸«à¸²à¹ƒà¸™ output folders
    output_dir = project_root / "output"
    if output_dir.exists():
        for item in output_dir.iterdir():
            if item.is_dir():
                datasets["output"].append(item)
                print(f"   ğŸ“ˆ Found output dataset: {item.name}")
    
    return datasets

def analyze_dataset_structure(dataset_path: Path) -> Dict:
    """à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¸­à¸‡ dataset"""
    
    analysis = {
        "path": str(dataset_path),
        "name": dataset_path.name,
        "total_size": 0,
        "directories": [],
        "label_files": [],
        "image_folders": [],
        "config_files": [],
        "sample_count": 0
    }
    
    if not dataset_path.exists():
        return analysis
    
    print(f"\nğŸ“Š Analyzing: {dataset_path.name}")
    
    # à¸«à¸²à¹„à¸Ÿà¸¥à¹Œà¹à¸¥à¸°à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸ªà¸³à¸„à¸±à¸
    for root, dirs, files in os.walk(dataset_path):
        root_path = Path(root)
        
        # à¸™à¸±à¸šà¸‚à¸™à¸²à¸”à¹„à¸Ÿà¸¥à¹Œ
        for file in files:
            file_path = root_path / file
            try:
                analysis["total_size"] += file_path.stat().st_size
            except:
                pass
            
            # à¸«à¸²à¹„à¸Ÿà¸¥à¹Œ label
            if file.endswith(('.txt', '.json')) and any(keyword in file.lower() 
                for keyword in ['label', 'gt', 'train', 'val', 'test']):
                analysis["label_files"].append(str(file_path.relative_to(dataset_path)))
                
                # à¸™à¸±à¸šà¸ˆà¸³à¸™à¸§à¸™à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¹ƒà¸™à¹„à¸Ÿà¸¥à¹Œ label
                try:
                    if file.endswith('.txt'):
                        with open(file_path, 'r', encoding='utf-8') as f:
                            lines = f.readlines()
                            analysis["sample_count"] += len([l for l in lines if l.strip()])
                except:
                    pass
            
            # à¸«à¸²à¹„à¸Ÿà¸¥à¹Œ config
            if file.endswith(('.yml', '.yaml', '.json', '.cfg')):
                analysis["config_files"].append(str(file_path.relative_to(dataset_path)))
        
        # à¸«à¸²à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸£à¸¹à¸›à¸ à¸²à¸
        for dir_name in dirs:
            dir_path = root_path / dir_name
            if any(keyword in dir_name.lower() for keyword in ['image', 'img', 'pic', 'train', 'val', 'test']):
                # à¸™à¸±à¸šà¸ˆà¸³à¸™à¸§à¸™à¸£à¸¹à¸›à¹ƒà¸™à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ
                try:
                    image_count = len([f for f in os.listdir(dir_path) 
                                     if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))])
                    if image_count > 0:
                        analysis["image_folders"].append({
                            "path": str(dir_path.relative_to(dataset_path)),
                            "image_count": image_count
                        })
                except:
                    pass
    
    # à¹à¸›à¸¥à¸‡à¸‚à¸™à¸²à¸”à¹ƒà¸«à¹‰à¸­à¹ˆà¸²à¸™à¸‡à¹ˆà¸²à¸¢
    size_mb = analysis["total_size"] / (1024 * 1024)
    print(f"   ğŸ“ Size: {size_mb:.1f} MB")
    print(f"   ğŸ“„ Label files: {len(analysis['label_files'])}")
    print(f"   ğŸ–¼ï¸ Image folders: {len(analysis['image_folders'])}")
    print(f"   ğŸ“Š Sample count: {analysis['sample_count']}")
    
    return analysis

def find_best_dataset_for_testing() -> Optional[Path]:
    """à¸«à¸² dataset à¸—à¸µà¹ˆà¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡à¸—à¸µà¹ˆà¸ªà¸¸à¸”à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£à¸—à¸”à¸ªà¸­à¸š"""
    
    datasets = find_datasets()
    
    if not datasets["converted"]:
        print("âŒ No converted datasets found!")
        return None
    
    print(f"\nğŸ¯ Found {len(datasets['converted'])} converted dataset(s)")
    
    best_dataset = None
    best_score = 0
    
    for dataset_path in datasets["converted"]:
        analysis = analyze_dataset_structure(dataset_path)
        
        # à¸„à¸³à¸™à¸§à¸“à¸„à¸°à¹à¸™à¸™à¸•à¸²à¸¡ criteria
        score = 0
        
        # à¸¡à¸µ label files à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ
        if analysis["label_files"]:
            score += 10
            
        # à¸¡à¸µà¸£à¸¹à¸›à¸ à¸²à¸à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ
        if analysis["image_folders"]:
            score += 10
            
        # à¸ˆà¸³à¸™à¸§à¸™ samples
        if analysis["sample_count"] > 0:
            score += min(analysis["sample_count"] / 100, 10)  # max 10 points
            
        # à¸‚à¸™à¸²à¸”à¹„à¸Ÿà¸¥à¹Œ (à¹„à¸¡à¹ˆà¹ƒà¸«à¸à¹ˆà¹€à¸à¸´à¸™à¹„à¸›)
        size_mb = analysis["total_size"] / (1024 * 1024)
        if 10 < size_mb < 500:  # à¸‚à¸™à¸²à¸”à¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡
            score += 5
        
        print(f"   ğŸ† Score for {dataset_path.name}: {score:.1f}")
        
        if score > best_score:
            best_score = score
            best_dataset = dataset_path
    
    if best_dataset:
        print(f"\nâœ… Best dataset for testing: {best_dataset.name}")
        print(f"   ğŸ“ Path: {best_dataset}")
    
    return best_dataset

def main():
    """à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸«à¸¥à¸±à¸"""
    print("ğŸ” Thai OCR Dataset Detection")
    print("=" * 50)
    
    best_dataset = find_best_dataset_for_testing()
    
    if best_dataset:
        print(f"\nğŸ“‹ Detailed analysis of recommended dataset:")
        detailed_analysis = analyze_dataset_structure(best_dataset)
        
        print(f"\nğŸ¯ Ready for testing!")
        print(f"   Use this path in your testing script:")
        print(f"   {best_dataset}")
        
        # à¹à¸ªà¸”à¸‡à¹„à¸Ÿà¸¥à¹Œ label à¸—à¸µà¹ˆà¸à¸š
        if detailed_analysis["label_files"]:
            print(f"\nğŸ“„ Available label files:")
            for label_file in detailed_analysis["label_files"]:
                print(f"   - {label_file}")
        
        # à¹à¸ªà¸”à¸‡à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸£à¸¹à¸›à¸—à¸µà¹ˆà¸à¸š
        if detailed_analysis["image_folders"]:
            print(f"\nğŸ–¼ï¸ Available image folders:")
            for img_folder in detailed_analysis["image_folders"]:
                print(f"   - {img_folder['path']} ({img_folder['image_count']} images)")
    
    else:
        print(f"\nâŒ No suitable dataset found for testing!")
        print(f"   Please check if datasets are properly converted to PaddleOCR format.")

if __name__ == "__main__":
    main()
