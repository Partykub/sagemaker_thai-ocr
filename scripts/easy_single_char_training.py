#!/usr/bin/env python3
"""
üöÄ Easy Thai OCR Single Character Training Workflow
‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏à‡∏ö: ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‚Üí ‡πÄ‡∏ó‡∏£‡∏ô ‚Üí ‡∏ó‡∏î‡∏™‡∏≠‡∏ö (SINGLE CHARACTER ONLY)
"""

import os
import sys
import json
import logging
import subprocess
from pathlib import Path
from datetime import datetime
import boto3
import yaml

class EasySingleCharTraining:
    def __init__(self):
        self.setup_logging()
        self.project_root = Path(__file__).parent.parent
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
    def setup_logging(self):
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)

    def step_1_prepare_single_char_data(self):
        """üìä Step 1: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• single character"""
        self.logger.info("üéØ Step 1: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Single Character")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á single character dataset
        output_dir = self.project_root / f"thai-letters/datasets/single_char_{self.timestamp}"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ‡∏£‡∏±‡∏ô generator ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö single character
        self.logger.info("üìù ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• single character...")
        
        cmd = [
            "python", "thai-letters/quick_phase1_generator.py", 
            "10"  # ‡∏™‡∏£‡πâ‡∏≤‡∏á 10 samples ‡∏ï‡πà‡∏≠‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True, cwd=str(self.project_root))
        if result.returncode != 0:
            raise Exception(f"‚ùå ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {result.stderr}")
            
        # ‡∏´‡∏≤ dataset ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà
        datasets_dir = self.project_root / "thai-letters/datasets/converted"
        latest_dataset = None
        
        for dataset_dir in sorted(datasets_dir.glob("train_data_thai_paddleocr_*"), reverse=True):
            if dataset_dir.is_dir():
                latest_dataset = dataset_dir
                break
        
        if not latest_dataset:
            raise FileNotFoundError("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö dataset ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà")
            
        self.logger.info(f"‚úÖ ‡∏û‡∏ö dataset: {latest_dataset.name}")
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô single character format
        train_data_dir = latest_dataset / "train_data/rec"
        single_char_dir = self.project_root / f"thai-letters/datasets/single_char_{self.timestamp}/train_data/rec"
        single_char_dir.mkdir(parents=True, exist_ok=True)
        
        # ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç label files ‡πÄ‡∏õ‡πá‡∏ô single character
        import shutil
        
        # ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å images
        if (train_data_dir / "thai_data").exists():
            shutil.copytree(train_data_dir / "thai_data", single_char_dir / "thai_data")
        
        # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç label files ‡πÄ‡∏õ‡πá‡∏ô single character
        self._convert_to_single_char_labels(train_data_dir, single_char_dir)
        
        # ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å dictionary
        shutil.copy(self.project_root / "thai-letters/th_dict.txt", single_char_dir / "th_dict.txt")
        
        # ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏õ S3
        s3_bucket = "paddleocr-dev-data-bucket"
        s3_prefix = f"data/single_char_{self.timestamp}"
        
        self.logger.info(f"üì§ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏õ s3://{s3_bucket}/{s3_prefix}/")
        
        cmd = [
            "aws", "s3", "sync", 
            str(single_char_dir),
            f"s3://{s3_bucket}/{s3_prefix}/",
            "--quiet"
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode != 0:
            raise Exception(f"‚ùå ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î S3 ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {result.stderr}")
            
        self.logger.info("‚úÖ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
        return s3_bucket, s3_prefix
    
    def _convert_to_single_char_labels(self, source_dir, target_dir):
        """‡πÅ‡∏õ‡∏•‡∏á label files ‡πÄ‡∏õ‡πá‡∏ô single character"""
        self.logger.info("üîß ‡πÅ‡∏õ‡∏•‡∏á labels ‡πÄ‡∏õ‡πá‡∏ô single character...")
        
        label_files = ["rec_gt_train.txt", "rec_gt_val.txt"]
        
        for label_file in label_files:
            source_file = source_dir / label_file
            target_file = target_dir / label_file
            
            if source_file.exists():
                with open(source_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                
                single_char_lines = []
                for line in lines:
                    line = line.strip()
                    if '\t' in line:
                        img_path, text = line.split('\t', 1)
                        # ‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÅ‡∏£‡∏Å
                        if text:
                            first_char = text[0]
                            single_char_lines.append(f"{img_path}\t{first_char}\n")
                
                with open(target_file, 'w', encoding='utf-8') as f:
                    f.writelines(single_char_lines)
                    
                self.logger.info(f"‚úÖ ‡πÅ‡∏õ‡∏•‡∏á {label_file}: {len(single_char_lines)} samples")

    def step_2_create_single_char_config(self):
        """‚öôÔ∏è Step 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á single character training config"""
        self.logger.info("üéØ Step 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á Single Character Training Configuration")
        
        config = {
            "Global": {
                "use_gpu": False,
                "epoch_num": 100,  # ‡πÄ‡∏û‡∏¥‡πà‡∏° epochs ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö single char
                "log_smooth_window": 20,
                "print_batch_step": 10,
                "save_model_dir": "/opt/ml/model/",
                "save_epoch_step": 20,
                "eval_batch_step": [0, 1000],
                "cal_metric_during_train": True,
                "character_dict_path": "/opt/ml/input/data/training/th_dict.txt",
                "character_type": "thai",
                "max_text_length": 1,  # SINGLE CHARACTER ONLY
                "use_space_char": False,
                "distributed": False
            },
            "Architecture": {
                "model_type": "rec",
                "algorithm": "CRNN",
                "Backbone": {
                    "name": "MobileNetV3",
                    "scale": 0.5,
                    "model_name": "large"
                },
                "Neck": {
                    "name": "SequenceEncoder",
                    "encoder_type": "rnn", 
                    "hidden_size": 48  # ‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö single char
                },
                "Head": {
                    "name": "CTCHead",
                    "fc_decay": 0.00001
                }
            },
            "Loss": {"name": "CTCLoss"},
            "PostProcess": {"name": "CTCLabelDecode"},
            "Metric": {
                "name": "RecMetric",
                "main_indicator": "acc"
            },
            "Optimizer": {
                "name": "Adam",
                "beta1": 0.9,
                "beta2": 0.999,
                "lr": {
                    "name": "Cosine",
                    "learning_rate": 0.0005,  # ‡∏•‡∏î learning rate
                    "warmup_epoch": 10
                },
                "regularizer": {
                    "name": "L2",
                    "factor": 0.00003
                }
            },
            "Train": {
                "dataset": {
                    "name": "SimpleDataSet",
                    "data_dir": "/opt/ml/input/data/training/",
                    "label_file_list": ["/opt/ml/input/data/training/rec_gt_train.txt"],
                    "transforms": [
                        {"DecodeImage": {"img_mode": "BGR", "channel_first": False}},
                        {"CTCLabelEncode": {}},
                        {"RecResizeImg": {"image_shape": [3, 32, 100]}},  # ‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡∏†‡∏≤‡∏û
                        {"KeepKeys": {"keep_keys": ["image", "label", "length"]}}
                    ]
                },
                "loader": {
                    "shuffle": True,
                    "batch_size_per_card": 256,  # ‡πÄ‡∏û‡∏¥‡πà‡∏° batch size
                    "drop_last": True,
                    "num_workers": 4
                }
            },
            "Eval": {
                "dataset": {
                    "name": "SimpleDataSet", 
                    "data_dir": "/opt/ml/input/data/training/",
                    "label_file_list": ["/opt/ml/input/data/training/rec_gt_val.txt"],
                    "transforms": [
                        {"DecodeImage": {"img_mode": "BGR", "channel_first": False}},
                        {"CTCLabelEncode": {}},
                        {"RecResizeImg": {"image_shape": [3, 32, 100]}},
                        {"KeepKeys": {"keep_keys": ["image", "label", "length"]}}
                    ]
                },
                "loader": {
                    "shuffle": False,
                    "batch_size_per_card": 256,
                    "drop_last": False,
                    "num_workers": 4
                }
            }
        }
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å config
        config_path = self.project_root / f"configs/rec/single_char_{self.timestamp}.yml"
        config_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
            
        self.logger.info(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á single char config: {config_path}")
        return config_path

    def step_3_start_training(self, s3_bucket, s3_prefix, config_path):
        """üöÄ Step 3: ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô single character ‡∏ö‡∏ô SageMaker"""
        self.logger.info("üéØ Step 3: ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô Single Character ‡∏ö‡∏ô SageMaker")
        
        sagemaker = boto3.client('sagemaker', region_name='ap-southeast-1')
        
        job_name = f"thai-single-char-{self.timestamp}"
        
        training_job_params = {
            'TrainingJobName': job_name,
            'AlgorithmSpecification': {
                'TrainingImage': '484468818942.dkr.ecr.ap-southeast-1.amazonaws.com/paddleocr-dev:latest',
                'TrainingInputMode': 'File'
            },
            'RoleArn': 'arn:aws:iam::484468818942:role/paddleocr-dev-sagemaker-role',
            'InputDataConfig': [{
                'ChannelName': 'training',
                'DataSource': {
                    'S3DataSource': {
                        'S3DataType': 'S3Prefix',
                        'S3Uri': f's3://{s3_bucket}/{s3_prefix}/',
                        'S3DataDistributionType': 'FullyReplicated'
                    }
                }
            }],
            'OutputDataConfig': {
                'S3OutputPath': f's3://{s3_bucket}/models/'
            },
            'ResourceConfig': {
                'InstanceType': 'ml.m5.large',
                'InstanceCount': 1,
                'VolumeSizeInGB': 30
            },
            'StoppingCondition': {
                'MaxRuntimeInSeconds': 10800  # 3 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á
            },
            'HyperParameters': {
                'epochs': '100',
                'batch-size': '256',
                'learning-rate': '0.0005',
                'max-text-length': '1',
                'config': config_path.name
            }
        }
        
        response = sagemaker.create_training_job(**training_job_params)
        
        self.logger.info(f"‚úÖ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô Single Character ‡πÅ‡∏•‡πâ‡∏ß: {job_name}")
        self.logger.info(f"üìç Job ARN: {response['TrainingJobArn']}")
        
        return job_name

    def step_4_monitor_training(self, job_name):
        """üìä Step 4: ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô"""
        self.logger.info("üéØ Step 4: ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô Single Character")
        
        sagemaker = boto3.client('sagemaker', region_name='ap-southeast-1')
        
        import time
        start_time = time.time()
        
        while True:
            response = sagemaker.describe_training_job(TrainingJobName=job_name)
            status = response['TrainingJobStatus']
            
            elapsed = int(time.time() - start_time)
            elapsed_str = f"{elapsed//3600:02d}:{(elapsed%3600)//60:02d}:{elapsed%60:02d}"
            
            self.logger.info(f"üìä ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞: {status} | ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏õ: {elapsed_str}")
            
            if status in ['Completed', 'Failed', 'Stopped']:
                break
                
            time.sleep(60)  # ‡∏£‡∏≠ 1 ‡∏ô‡∏≤‡∏ó‡∏µ
            
        if status == 'Completed':
            self.logger.info("üéâ ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô Single Character ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
            return response['ModelArtifacts']['S3ModelArtifacts']
        else:
            raise Exception(f"‚ùå ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {status}")

    def step_5_download_model(self, model_s3_path):
        """üì• Step 5: ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• single character"""
        self.logger.info("üéØ Step 5: ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• Single Character")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á directory ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏°‡πà
        model_dir = self.project_root / f"models/single_char_{self.timestamp}"
        model_dir.mkdir(parents=True, exist_ok=True)
        
        # ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å S3
        cmd = [
            "aws", "s3", "cp", 
            model_s3_path,
            str(model_dir / "model.tar.gz")
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode != 0:
            raise Exception(f"‚ùå ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {result.stderr}")
            
        # ‡πÅ‡∏ï‡∏Å‡πÑ‡∏ü‡∏•‡πå
        import tarfile
        with tarfile.open(model_dir / "model.tar.gz", 'r:gz') as tar:
            tar.extractall(model_dir)
            
        self.logger.info(f"‚úÖ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• Single Character ‡πÅ‡∏•‡πâ‡∏ß: {model_dir}")
        return model_dir

    def step_6_test_single_char_model(self, model_dir):
        """üß™ Step 6: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• single character"""
        self.logger.info("üéØ Step 6: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• Single Character")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á inference config ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö single character
        inference_config = {
            "Global": {
                "use_gpu": False,
                "pretrained_model": str(model_dir / "best_accuracy"),
                "character_dict_path": str(self.project_root / "thai-letters/th_dict.txt"),
                "character_type": "thai",
                "max_text_length": 1,  # SINGLE CHARACTER
                "infer_mode": True,
                "use_space_char": False
            },
            "Architecture": {
                "model_type": "rec",
                "algorithm": "CRNN",
                "Backbone": {
                    "name": "MobileNetV3",
                    "scale": 0.5,
                    "model_name": "large"
                },
                "Neck": {
                    "name": "SequenceEncoder",
                    "encoder_type": "rnn",
                    "hidden_size": 48
                },
                "Head": {
                    "name": "CTCHead",
                    "fc_decay": 0.00001
                }
            },
            "PostProcess": {"name": "CTCLabelDecode"}
        }
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å config
        inference_config_path = model_dir / "single_char_inference_config.yml"
        with open(inference_config_path, 'w', encoding='utf-8') as f:
            yaml.dump(inference_config, f, default_flow_style=False, allow_unicode=True)
            
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏†‡∏≤‡∏û‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
        test_images_dir = self.project_root / "test_images"
        if test_images_dir.exists():
            test_images = list(test_images_dir.glob("*.jpg"))[:10]
            
            self.logger.info(f"üß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Single Character ‡∏Å‡∏±‡∏ö {len(test_images)} ‡∏†‡∏≤‡∏û")
            
            results = []
            for img_path in test_images:
                cmd = [
                    "python", "PaddleOCR/tools/infer_rec.py",
                    "-c", str(inference_config_path),
                    "-o", f"Global.infer_img={img_path}"
                ]
                
                result = subprocess.run(cmd, capture_output=True, text=True, cwd=str(self.project_root))
                
                if "result:" in result.stdout:
                    prediction = result.stdout.split("result:")[1].split("\n")[0].strip()
                    # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö single char ‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å
                    if prediction:
                        single_char = prediction.split()[0] if ' ' in prediction else prediction[:1]
                        results.append({
                            "image": img_path.name,
                            "prediction": single_char,
                            "raw": prediction
                        })
                        self.logger.info(f"üì∑ {img_path.name}: '{single_char}'")
                    
            # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            self.logger.info(f"‚úÖ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Single Character ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô: {len(results)} ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå")
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            results_file = model_dir / "test_results.json"
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
                
            return results

    def run_complete_workflow(self):
        """üéØ ‡∏£‡∏±‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÅ‡∏ö‡∏ö One-Click ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Single Character"""
        try:
            self.logger.info("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Easy Thai OCR Single Character Training Workflow")
            
            # Step 1: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• single character
            s3_bucket, s3_prefix = self.step_1_prepare_single_char_data()
            
            # Step 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á single char config
            config_path = self.step_2_create_single_char_config()
            
            # Step 3: ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô
            job_name = self.step_3_start_training(s3_bucket, s3_prefix, config_path)
            
            # Step 4: ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô
            model_s3_path = self.step_4_monitor_training(job_name)
            
            # Step 5: ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•
            model_dir = self.step_5_download_model(model_s3_path)
            
            # Step 6: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• single character
            test_results = self.step_6_test_single_char_model(model_dir)
            
            self.logger.info("üéâ ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! ‡πÇ‡∏°‡πÄ‡∏î‡∏• Thai OCR Single Character ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
            
            return {
                "status": "success",
                "job_name": job_name,
                "model_path": str(model_dir),
                "timestamp": self.timestamp,
                "test_results": test_results,
                "model_type": "single_character"
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
            return {
                "status": "failed",
                "error": str(e),
                "timestamp": self.timestamp,
                "model_type": "single_character"
            }

def main():
    """Main function"""
    print("üéØ Easy Thai OCR Single Character Training")
    print("=" * 60)
    
    trainer = EasySingleCharTraining()
    result = trainer.run_complete_workflow()
    
    print(f"\nüéØ ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:")
    print(json.dumps(result, indent=2, ensure_ascii=False))

if __name__ == "__main__":
    main()
