# AWS Infrastructure Rules

## AWS Service Guidelines

### SageMaker Best Practices
```python
# SageMaker training job configuration
import sagemaker
from sagemaker.pytorch import PyTorch

def create_training_job(
    role: str,
    image_uri: str,
    instance_type: str = "ml.p3.2xlarge",
    instance_count: int = 1
) -> PyTorch:
    """Create SageMaker training job with proper configuration."""
    estimator = PyTorch(
        entry_point="train.py",
        source_dir="scripts/training",
        role=role,
        image_uri=image_uri,
        instance_type=instance_type,
        instance_count=instance_count,
        framework_version="1.12.0",
        py_version="py38",
        hyperparameters={
            "epochs": 100,
            "batch_size": 32,
            "learning_rate": 0.001
        },
        environment={
            "PYTHONPATH": "/opt/ml/code"
        }
    )
    return estimator
```

### S3 Data Management
```python
# S3 operations with proper error handling
import boto3
from botocore.exceptions import ClientError
from pathlib import Path

class S3DataManager:
    def __init__(self, bucket_name: str):
        self.bucket_name = bucket_name
        self.s3_client = boto3.client('s3')
    
    def upload_dataset(self, local_path: Path, s3_prefix: str) -> bool:
        """Upload dataset to S3 with proper structure."""
        try:
            for file_path in local_path.rglob("*"):
                if file_path.is_file():
                    relative_path = file_path.relative_to(local_path)
                    s3_key = f"{s3_prefix}/{relative_path}"
                    self.s3_client.upload_file(
                        str(file_path), 
                        self.bucket_name, 
                        s3_key
                    )
            return True
        except ClientError as e:
            logging.error(f"Failed to upload dataset: {e}")
            return False
```

### ECR Repository Management
```python
# ECR operations for Docker images
import boto3
import base64

def push_to_ecr(image_name: str, tag: str, region: str) -> str:
    """Push Docker image to ECR repository."""
    ecr_client = boto3.client('ecr', region_name=region)
    
    # Get authorization token
    token_response = ecr_client.get_authorization_token()
    token = token_response['authorizationData'][0]['authorizationToken']
    username, password = base64.b64decode(token).decode().split(':')
    
    # Build repository URI
    account_id = boto3.client('sts').get_caller_identity()['Account']
    repository_uri = f"{account_id}.dkr.ecr.{region}.amazonaws.com/{image_name}"
    
    return f"{repository_uri}:{tag}"
```

## Terraform Infrastructure Patterns

### Resource Naming Convention
```hcl
# Consistent naming across all resources
locals {
  name_prefix = "thai-ocr-${var.environment}"
  
  common_tags = {
    Project     = "thai-ocr"
    Environment = var.environment
    ManagedBy   = "terraform"
    Owner       = var.owner
    CostCenter  = var.cost_center
  }
}
```

### S3 Bucket Configuration
```hcl
resource "aws_s3_bucket" "data_bucket" {
  bucket = "${local.name_prefix}-data-${random_id.bucket_suffix.hex}"
  tags   = local.common_tags
}

resource "aws_s3_bucket_versioning" "data_bucket_versioning" {
  bucket = aws_s3_bucket.data_bucket.id
  versioning_configuration {
    status = "Enabled"
  }
}

resource "aws_s3_bucket_server_side_encryption_configuration" "data_bucket_encryption" {
  bucket = aws_s3_bucket.data_bucket.id

  rule {
    apply_server_side_encryption_by_default {
      sse_algorithm = "AES256"
    }
  }
}
```

### IAM Roles and Policies
```hcl
# SageMaker execution role
resource "aws_iam_role" "sagemaker_execution_role" {
  name = "${local.name_prefix}-sagemaker-execution-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "sagemaker.amazonaws.com"
        }
      }
    ]
  })

  tags = local.common_tags
}

resource "aws_iam_role_policy_attachment" "sagemaker_execution_role_policy" {
  role       = aws_iam_role.sagemaker_execution_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonSageMakerFullAccess"
}
```

### Lambda Functions
```hcl
resource "aws_lambda_function" "training_orchestrator" {
  filename         = "training_orchestrator.zip"
  function_name    = "${local.name_prefix}-training-orchestrator"
  role            = aws_iam_role.lambda_execution_role.arn
  handler         = "lambda_function.lambda_handler"
  runtime         = "python3.9"
  timeout         = 900

  environment {
    variables = {
      SAGEMAKER_ROLE_ARN = aws_iam_role.sagemaker_execution_role.arn
      S3_BUCKET         = aws_s3_bucket.data_bucket.bucket
      ECR_REPOSITORY    = aws_ecr_repository.thai_ocr.repository_url
    }
  }

  tags = local.common_tags
}
```

## Security Best Practices

### IAM Least Privilege
```hcl
# Custom policy for specific S3 access
resource "aws_iam_policy" "s3_data_access" {
  name        = "${local.name_prefix}-s3-data-access"
  description = "Access to Thai OCR data bucket"

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "s3:GetObject",
          "s3:PutObject",
          "s3:DeleteObject"
        ]
        Resource = "${aws_s3_bucket.data_bucket.arn}/*"
      },
      {
        Effect = "Allow"
        Action = [
          "s3:ListBucket"
        ]
        Resource = aws_s3_bucket.data_bucket.arn
      }
    ]
  })
}
```

### VPC Configuration
```hcl
# VPC for secure SageMaker training
resource "aws_vpc" "main" {
  cidr_block           = "10.0.0.0/16"
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = merge(local.common_tags, {
    Name = "${local.name_prefix}-vpc"
  })
}

resource "aws_subnet" "private" {
  count             = 2
  vpc_id            = aws_vpc.main.id
  cidr_block        = "10.0.${count.index + 1}.0/24"
  availability_zone = data.aws_availability_zones.available.names[count.index]

  tags = merge(local.common_tags, {
    Name = "${local.name_prefix}-private-subnet-${count.index + 1}"
  })
}
```

## Monitoring and Logging

### CloudWatch Configuration
```hcl
resource "aws_cloudwatch_log_group" "sagemaker_training" {
  name              = "/aws/sagemaker/TrainingJobs/${local.name_prefix}"
  retention_in_days = 14

  tags = local.common_tags
}

resource "aws_cloudwatch_metric_alarm" "training_failure" {
  alarm_name          = "${local.name_prefix}-training-failure"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = "1"
  metric_name         = "TrainingJobsFailed"
  namespace           = "AWS/SageMaker"
  period              = "300"
  statistic           = "Sum"
  threshold           = "0"
  alarm_description   = "This metric monitors SageMaker training job failures"

  tags = local.common_tags
}
```

## Cost Optimization

### Resource Scheduling
```python
# Lambda function for stopping/starting resources
import boto3
from datetime import datetime

def lambda_handler(event, context):
    """Stop non-production resources during off-hours."""
    current_hour = datetime.now().hour
    
    # Stop SageMaker endpoints during off-hours (10 PM - 6 AM)
    if current_hour >= 22 or current_hour <= 6:
        sagemaker = boto3.client('sagemaker')
        
        # List and stop endpoints
        endpoints = sagemaker.list_endpoints(
            StatusEquals='InService',
            NameContains='thai-ocr-dev'
        )
        
        for endpoint in endpoints['Endpoints']:
            sagemaker.delete_endpoint(
                EndpointName=endpoint['EndpointName']
            )
```

### Spot Instance Configuration
```hcl
# Use Spot instances for training jobs
resource "aws_sagemaker_model" "thai_ocr_model" {
  name               = "${local.name_prefix}-model"
  execution_role_arn = aws_iam_role.sagemaker_execution_role.arn

  primary_container {
    image = aws_ecr_repository.thai_ocr.repository_url
  }

  tags = local.common_tags
}
```
