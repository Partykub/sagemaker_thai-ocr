# Project Specification Rules

## Project Overview

- **Objective**: Build a robust OCR engine for Thai language using PaddleOCR, with support for custom training, AWS SageMaker deployment, and Terraform-managed infrastructure.
- **Main Components**:
  - **thai-letters/**: Python and batch scripts for synthetic and real Thai text image generation and annotation
  - **train_data_thai_paddleocr_.../**: Converted dataset ready for PaddleOCR training (image/, label/, train_list.txt, val_list.txt)
  - **configs/**: YAML configuration files for detection and recognition models (e.g., `thai_svtr_tiny_config.yml`)
  - **ppocr/utils/dict/**: Thai character dictionary (`th_dict.txt`)
  - **doc/**: Comprehensive documentation in Markdown format
  - **terraform/**: Infrastructure as Code for AWS resources

## Usage Context

### Data Generation
- Generate synthetic Thai text images with various fonts and styles
- Create real Thai text annotations and labels
- Convert datasets to PaddleOCR format with proper structure

### Dataset Conversion
- Transform annotated images to PaddleOCR training format
- Generate train_list.txt and val_list.txt files
- Organize images and labels in proper directory structure

### Model Training
- **Local Training**: Using `PaddleOCR/tools/train.py` with custom configurations
- **SageMaker Training**: Via AWS Lambda or Step Functions, containerized in Docker, pushed to ECR
- **Distributed Training**: Multi-GPU and multi-node training on SageMaker

### Inference & Deployment
- **Local Inference**: Using Python scripts and PaddleOCR API
- **Real-time Inference**: Via SageMaker endpoints for live OCR processing
- **Batch Inference**: For processing large volumes of documents

### Infrastructure as Code
- Terraform scripts for S3 buckets, ECR repositories, IAM roles
- Lambda functions for training orchestration
- SageMaker resources for training and inference

## Amazon Q Code Generation Rules

### Python Code Standards
```python
# Always include proper imports
import logging
from typing import Dict, List, Optional, Tuple
import boto3
from pathlib import Path

# Use type hints for all functions
def process_thai_text(
    text: str, 
    config: Dict[str, Any],
    output_path: Optional[Path] = None
) -> Tuple[bool, str]:
    """Process Thai text with proper error handling."""
    try:
        # Implementation here
        logging.info(f"Processing text: {text[:50]}...")
        return True, "Success"
    except Exception as e:
        logging.error(f"Error processing text: {e}")
        return False, str(e)
```

### AWS SDK Best Practices
```python
# Use boto3 sessions with proper error handling
import boto3
from botocore.exceptions import ClientError

def upload_to_s3(file_path: str, bucket: str, key: str) -> bool:
    """Upload file to S3 with proper error handling."""
    try:
        s3_client = boto3.client('s3')
        s3_client.upload_file(file_path, bucket, key)
        logging.info(f"Successfully uploaded {file_path} to s3://{bucket}/{key}")
        return True
    except ClientError as e:
        logging.error(f"Failed to upload to S3: {e}")
        return False
```

### Terraform Code Standards
```hcl
# Use consistent resource naming and tagging
resource "aws_s3_bucket" "thai_ocr_data" {
  bucket = "thai-ocr-${var.environment}-data-${random_id.bucket_suffix.hex}"

  tags = {
    Name        = "Thai OCR Data Bucket"
    Environment = var.environment
    Project     = "thai-ocr"
    ManagedBy   = "terraform"
  }
}

# Include proper variables and outputs
variable "environment" {
  description = "Environment name (dev, staging, prod)"
  type        = string
  validation {
    condition     = contains(["dev", "staging", "prod"], var.environment)
    error_message = "Environment must be dev, staging, or prod."
  }
}
```

### Configuration File Standards
```yaml
# PaddleOCR configuration with proper structure
Global:
  use_gpu: true
  epoch_num: 100
  log_smooth_window: 20
  print_batch_step: 10
  save_model_dir: ./output/thai_rec/
  save_epoch_step: 10
  eval_batch_step: 500
  cal_metric_during_train: true
  pretrained_model: null
  checkpoints: null
  save_inference_dir: null
  use_visualdl: false
  infer_img: doc/imgs_words/ch/
  character_dict_path: ppocr/utils/dict/th_dict.txt
  character_type: th
  max_text_length: 25
  infer_mode: false
  use_space_char: true
  distributed: true
```

## Thai Language Specific Rules

### Character Handling
- Always use UTF-8 encoding for Thai text processing
- Handle Thai character complexity (vowels, tone marks, consonants)
- Consider Thai text rendering and font requirements
- Implement proper Thai text validation

### OCR Processing
- Account for Thai character stacking and positioning
- Handle Thai punctuation and spacing correctly
- Consider Thai reading direction and layout
- Implement Thai-specific post-processing rules

### Font and Rendering
```python
# Thai font handling example
THAI_FONTS = [
    "TH Sarabun New",
    "Angsana New", 
    "Cordia New",
    "DilleniaUPC",
    "EucrosiaUPC"
]

def generate_thai_text_image(text: str, font_name: str) -> Image:
    """Generate Thai text image with proper font rendering."""
    # Implementation with proper Thai font handling
    pass
```

## Common Development Tasks

### Data Pipeline Tasks
- Create or update data generators in `thai-letters/`
- Implement data validation and quality checks
- Optimize dataset conversion for PaddleOCR format
- Handle large-scale data processing efficiently

### Model Training Tasks
- Update or extend PaddleOCR configurations under `configs/rec/`
- Implement hyperparameter tuning strategies
- Set up distributed training on SageMaker
- Monitor training progress and metrics

### Deployment Tasks
- Create SageMaker training scripts and Lambda handlers
- Implement model versioning and artifact management
- Set up real-time and batch inference endpoints
- Configure auto-scaling and monitoring

### Infrastructure Tasks
- Design and implement Terraform modules for AWS components
- Set up CI/CD pipelines for automated deployment
- Implement proper security and access controls
- Configure cost optimization and monitoring

## Error Handling and Logging

### Standard Error Handling
```python
import logging
from typing import Optional

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def safe_operation(data: Any) -> Optional[Any]:
    """Template for safe operations with proper error handling."""
    try:
        # Operation logic here
        result = process_data(data)
        logger.info("Operation completed successfully")
        return result
    except ValueError as e:
        logger.error(f"Invalid data provided: {e}")
        return None
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        raise
```

### AWS Error Handling
```python
from botocore.exceptions import ClientError, NoCredentialsError

def aws_operation_with_retry(operation_func, max_retries: int = 3):
    """AWS operation with retry logic."""
    for attempt in range(max_retries):
        try:
            return operation_func()
        except ClientError as e:
            error_code = e.response['Error']['Code']
            if error_code in ['Throttling', 'ServiceUnavailable'] and attempt < max_retries - 1:
                time.sleep(2 ** attempt)  # Exponential backoff
                continue
            logger.error(f"AWS operation failed: {e}")
            raise
        except NoCredentialsError:
            logger.error("AWS credentials not found")
            raise
```
