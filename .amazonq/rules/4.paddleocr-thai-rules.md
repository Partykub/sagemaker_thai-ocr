# PaddleOCR and Thai Language Rules

## PaddleOCR Configuration Standards

### Recognition Model Configuration
```yaml
# Standard Thai recognition model config
Global:
  use_gpu: true
  epoch_num: 100
  log_smooth_window: 20
  print_batch_step: 10
  save_model_dir: ./output/thai_rec/
  save_epoch_step: 10
  eval_batch_step: 500
  cal_metric_during_train: true
  pretrained_model: null
  checkpoints: null
  save_inference_dir: null
  use_visualdl: false
  infer_img: doc/imgs_words/th/
  character_dict_path: ppocr/utils/dict/th_dict.txt
  character_type: th
  max_text_length: 25
  infer_mode: false
  use_space_char: true
  distributed: true

Architecture:
  model_type: rec
  algorithm: SVTR_LCNet
  Transform:
  Backbone:
    name: SVTRNet
    img_size: [64, 256]
    out_char_num: 25
    out_channels: 192
    patch_merging: 'Conv'
    embed_dim: [64, 128, 256]
    depth: [3, 6, 3]
    num_heads: [2, 4, 8]
    mixer: ['Local','Local','Local','Local','Local','Local','Global','Global','Global','Global','Global','Global']
    local_mixer: [[7, 11], [7, 11], [7, 11]]
    last_stage: true
    prenorm: false
  Head:
    name: CTCHead
    fc_decay: 0.00001

Loss:
  name: CTCLoss

Optimizer:
  name: Adam
  beta1: 0.9
  beta2: 0.999
  lr:
    name: Cosine
    learning_rate: 0.001
    warmup_epoch: 5
  regularizer:
    name: 'L2'
    factor: 0.00001

PostProcess:
  name: CTCLabelDecode

Metric:
  name: RecMetric
  main_indicator: acc

Train:
  dataset:
    name: SimpleDataSet
    data_dir: ./train_data_thai_paddleocr_v1/
    label_file_list:
      - ./train_data_thai_paddleocr_v1/train_list.txt
    transforms:
      - DecodeImage:
          img_mode: BGR
          channel_first: false
      - CTCLabelEncode:
      - RecResizeImg:
          image_shape: [3, 64, 256]
      - KeepKeys:
          keep_keys: ['image', 'label', 'length']
  loader:
    shuffle: true
    batch_size_per_card: 256
    drop_last: true
    num_workers: 8

Eval:
  dataset:
    name: SimpleDataSet
    data_dir: ./train_data_thai_paddleocr_v1/
    label_file_list:
      - ./train_data_thai_paddleocr_v1/val_list.txt
    transforms:
      - DecodeImage:
          img_mode: BGR
          channel_first: false
      - CTCLabelEncode:
      - RecResizeImg:
          image_shape: [3, 64, 256]
      - KeepKeys:
          keep_keys: ['image', 'label', 'length']
  loader:
    shuffle: false
    drop_last: false
    batch_size_per_card: 256
    num_workers: 4
```

### Detection Model Configuration
```yaml
# Thai text detection configuration
Global:
  use_gpu: true
  epoch_num: 1200
  log_smooth_window: 20
  print_batch_step: 2
  save_model_dir: ./output/thai_det/
  save_epoch_step: 200
  eval_batch_step: 2000
  cal_metric_during_train: false
  pretrained_model: ./pretrain_models/MobileNetV3_large_x0_5_pretrained
  checkpoints:
  save_inference_dir:
  use_visualdl: false
  infer_img: doc/imgs_en/img_10.jpg
  save_res_path: ./output/det_db/predicts_db.txt

Architecture:
  model_type: det
  algorithm: DB
  Transform:
  Backbone:
    name: MobileNetV3
    scale: 0.5
    model_name: large
    disable_se: true
  Neck:
    name: DBFPN
    out_channels: 256
  Head:
    name: DBHead
    k: 50

Loss:
  name: DBLoss
  balance_loss: true
  main_loss_type: DiceLoss
  alpha: 5
  beta: 10
  ohem_ratio: 3

Optimizer:
  name: Adam
  beta1: 0.9
  beta2: 0.999
  lr:
    name: Cosine
    learning_rate: 0.001
    warmup_epoch: 2
  regularizer:
    name: 'L2'
    factor: 0

PostProcess:
  name: DBPostProcess
  thresh: 0.3
  box_thresh: 0.6
  max_candidates: 1000
  unclip_ratio: 1.5

Metric:
  name: DetMetric
  main_indicator: hmean
```

## Thai Language Processing Rules

### Character Set Management
```python
# Thai character dictionary management
THAI_CHARACTERS = {
    'consonants': [
        'ก', 'ข', 'ฃ', 'ค', 'ฅ', 'ฆ', 'ง', 'จ', 'ฉ', 'ช', 'ซ', 'ฌ', 'ญ', 
        'ฎ', 'ฏ', 'ฐ', 'ฑ', 'ฒ', 'ณ', 'ด', 'ต', 'ถ', 'ท', 'ธ', 'น', 'บ', 
        'ป', 'ผ', 'ฝ', 'พ', 'ฟ', 'ภ', 'ม', 'ย', 'ร', 'ล', 'ว', 'ศ', 'ษ', 
        'ส', 'ห', 'ฬ', 'อ', 'ฮ'
    ],
    'vowels': [
        'ะ', 'ั', 'า', 'ำ', 'ิ', 'ี', 'ึ', 'ื', 'ุ', 'ู', 'เ', 'แ', 'โ', 
        'ใ', 'ไ', 'ๅ', 'ๆ', '็', '่', '้', '๊', '๋', '์'
    ],
    'numbers': ['๐', '๑', '๒', '๓', '๔', '๕', '๖', '๗', '๘', '๙'],
    'punctuation': ['ฯ', '๏', '๚', '๛'],
    'special': [' ', '\n', '\t']
}

def create_thai_dict(output_path: str) -> None:
    """Create comprehensive Thai character dictionary."""
    all_chars = []
    all_chars.extend(THAI_CHARACTERS['consonants'])
    all_chars.extend(THAI_CHARACTERS['vowels'])
    all_chars.extend(THAI_CHARACTERS['numbers'])
    all_chars.extend(THAI_CHARACTERS['punctuation'])
    all_chars.extend(THAI_CHARACTERS['special'])
    
    with open(output_path, 'w', encoding='utf-8') as f:
        for char in all_chars:
            f.write(f"{char}\n")
```

### Text Preprocessing
```python
import re
from typing import List, Tuple

def preprocess_thai_text(text: str) -> str:
    """Preprocess Thai text for OCR training."""
    # Remove excessive whitespace
    text = re.sub(r'\s+', ' ', text)
    
    # Normalize Thai characters
    text = text.replace('ำ', 'ํา')  # Normalize sara am
    
    # Remove non-Thai characters except numbers and basic punctuation
    thai_pattern = r'[ก-๙\s\.\,\!\?\-\(\)]'
    text = ''.join(re.findall(thai_pattern, text))
    
    return text.strip()

def validate_thai_text(text: str) -> Tuple[bool, List[str]]:
    """Validate Thai text for training data quality."""
    issues = []
    
    # Check for minimum length
    if len(text) < 2:
        issues.append("Text too short")
    
    # Check for maximum length
    if len(text) > 25:
        issues.append("Text too long for recognition model")
    
    # Check for invalid characters
    valid_chars = set(''.join(THAI_CHARACTERS.values()))
    invalid_chars = set(text) - valid_chars
    if invalid_chars:
        issues.append(f"Invalid characters: {invalid_chars}")
    
    return len(issues) == 0, issues
```

### Font and Image Generation
```python
from PIL import Image, ImageDraw, ImageFont
import numpy as np

class ThaiTextImageGenerator:
    def __init__(self, font_paths: List[str]):
        self.fonts = {}
        for font_path in font_paths:
            try:
                font_name = font_path.split('/')[-1].split('.')[0]
                self.fonts[font_name] = ImageFont.truetype(font_path, 32)
            except Exception as e:
                print(f"Failed to load font {font_path}: {e}")
    
    def generate_text_image(
        self, 
        text: str, 
        font_name: str,
        image_size: Tuple[int, int] = (256, 64),
        background_color: Tuple[int, int, int] = (255, 255, 255),
        text_color: Tuple[int, int, int] = (0, 0, 0)
    ) -> Image.Image:
        """Generate Thai text image with proper rendering."""
        
        if font_name not in self.fonts:
            raise ValueError(f"Font {font_name} not available")
        
        font = self.fonts[font_name]
        
        # Create image
        img = Image.new('RGB', image_size, background_color)
        draw = ImageDraw.Draw(img)
        
        # Calculate text position for centering
        bbox = draw.textbbox((0, 0), text, font=font)
        text_width = bbox[2] - bbox[0]
        text_height = bbox[3] - bbox[1]
        
        x = (image_size[0] - text_width) // 2
        y = (image_size[1] - text_height) // 2
        
        # Draw text
        draw.text((x, y), text, font=font, fill=text_color)
        
        return img
    
    def add_noise_and_distortion(self, img: Image.Image) -> Image.Image:
        """Add realistic noise and distortion to text images."""
        img_array = np.array(img)
        
        # Add Gaussian noise
        noise = np.random.normal(0, 10, img_array.shape)
        img_array = np.clip(img_array + noise, 0, 255).astype(np.uint8)
        
        # Add slight blur
        from scipy import ndimage
        img_array = ndimage.gaussian_filter(img_array, sigma=0.5)
        
        return Image.fromarray(img_array)
```

## Dataset Conversion Rules

### PaddleOCR Format Conversion
```python
import os
import json
from pathlib import Path
from typing import Dict, List, Tuple

class PaddleOCRConverter:
    def __init__(self, input_dir: str, output_dir: str):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Create subdirectories
        (self.output_dir / "images").mkdir(exist_ok=True)
        (self.output_dir / "labels").mkdir(exist_ok=True)
    
    def convert_dataset(self, train_ratio: float = 0.8) -> None:
        """Convert dataset to PaddleOCR format."""
        train_list = []
        val_list = []
        
        # Process all annotation files
        for ann_file in self.input_dir.glob("*.json"):
            with open(ann_file, 'r', encoding='utf-8') as f:
                annotations = json.load(f)
            
            for i, ann in enumerate(annotations):
                # Copy image
                img_name = f"{ann_file.stem}_{i:04d}.jpg"
                img_path = self.output_dir / "images" / img_name
                
                # Create label entry
                label_entry = f"images/{img_name}\t{ann['text']}"
                
                # Split train/val
                if np.random.random() < train_ratio:
                    train_list.append(label_entry)
                else:
                    val_list.append(label_entry)
        
        # Write list files
        with open(self.output_dir / "train_list.txt", 'w', encoding='utf-8') as f:
            f.write('\n'.join(train_list))
        
        with open(self.output_dir / "val_list.txt", 'w', encoding='utf-8') as f:
            f.write('\n'.join(val_list))
        
        print(f"Converted {len(train_list)} training samples and {len(val_list)} validation samples")
```

## Training Optimization

### Hyperparameter Tuning
```python
# Hyperparameter configurations for different scenarios
TRAINING_CONFIGS = {
    'quick_test': {
        'epoch_num': 10,
        'batch_size': 64,
        'learning_rate': 0.001,
        'save_epoch_step': 5
    },
    'development': {
        'epoch_num': 50,
        'batch_size': 128,
        'learning_rate': 0.001,
        'save_epoch_step': 10
    },
    'production': {
        'epoch_num': 200,
        'batch_size': 256,
        'learning_rate': 0.0005,
        'save_epoch_step': 20,
        'warmup_epoch': 10
    }
}

def update_config_for_training(config_path: str, training_type: str) -> None:
    """Update PaddleOCR config for specific training scenario."""
    import yaml
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    training_config = TRAINING_CONFIGS[training_type]
    
    # Update global settings
    config['Global'].update({
        'epoch_num': training_config['epoch_num'],
        'save_epoch_step': training_config['save_epoch_step']
    })
    
    # Update optimizer
    config['Optimizer']['lr']['learning_rate'] = training_config['learning_rate']
    if 'warmup_epoch' in training_config:
        config['Optimizer']['lr']['warmup_epoch'] = training_config['warmup_epoch']
    
    # Update batch size
    config['Train']['loader']['batch_size_per_card'] = training_config['batch_size']
    config['Eval']['loader']['batch_size_per_card'] = training_config['batch_size']
    
    # Save updated config
    with open(config_path, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
```
