# Testing and Quality Assurance Rules

## Testing Framework Standards

### Unit Testing Structure
```python
# tests/test_thai_text_processing.py
import pytest
import numpy as np
from pathlib import Path
from thai_letters.text_processor import ThaiTextProcessor, preprocess_thai_text

class TestThaiTextProcessor:
    """Test suite for Thai text processing functions."""
    
    def setup_method(self):
        """Setup test fixtures."""
        self.processor = ThaiTextProcessor()
        self.sample_texts = [
            "สวัสดีครับ",
            "ภาษาไทยสำหรับ OCR",
            "๑๒๓๔๕๖๗๘๙๐",
            "ก่อนหน้านี้"
        ]
    
    def test_preprocess_thai_text_basic(self):
        """Test basic Thai text preprocessing."""
        input_text = "  สวัสดี   ครับ  "
        expected = "สวัสดี ครับ"
        result = preprocess_thai_text(input_text)
        assert result == expected
    
    def test_preprocess_thai_text_normalization(self):
        """Test Thai character normalization."""
        input_text = "สำคัญ"  # Contains sara am
        result = preprocess_thai_text(input_text)
        assert "ำ" not in result or "ํา" in result
    
    def test_validate_thai_text_length(self):
        """Test text length validation."""
        short_text = "ก"
        long_text = "ก" * 30
        valid_text = "สวัสดี"
        
        assert not self.processor.validate_text(short_text)[0]
        assert not self.processor.validate_text(long_text)[0]
        assert self.processor.validate_text(valid_text)[0]
    
    @pytest.mark.parametrize("text", [
        "สวัสดี",
        "๑๒๓",
        "ก่อน",
        "ภาษาไทย"
    ])
    def test_valid_thai_texts(self, text):
        """Test various valid Thai texts."""
        is_valid, issues = self.processor.validate_text(text)
        assert is_valid, f"Text '{text}' should be valid, issues: {issues}"
```

### Integration Testing
```python
# tests/test_paddleocr_integration.py
import pytest
import tempfile
from pathlib import Path
from thai_letters.paddleocr_converter import PaddleOCRConverter

class TestPaddleOCRIntegration:
    """Test PaddleOCR integration and dataset conversion."""
    
    def setup_method(self):
        """Setup test environment."""
        self.temp_dir = tempfile.mkdtemp()
        self.input_dir = Path(self.temp_dir) / "input"
        self.output_dir = Path(self.temp_dir) / "output"
        self.input_dir.mkdir(parents=True)
    
    def test_dataset_conversion(self):
        """Test dataset conversion to PaddleOCR format."""
        # Create sample annotation file
        sample_annotations = [
            {"text": "สวัสดี", "image": "sample1.jpg"},
            {"text": "ครับ", "image": "sample2.jpg"}
        ]
        
        converter = PaddleOCRConverter(str(self.input_dir), str(self.output_dir))
        converter.convert_dataset()
        
        # Check output structure
        assert (self.output_dir / "train_list.txt").exists()
        assert (self.output_dir / "val_list.txt").exists()
        assert (self.output_dir / "images").exists()
    
    def test_config_validation(self):
        """Test PaddleOCR configuration validation."""
        config_path = "configs/rec/thai_rec_config.yml"
        # Test config loading and validation
        pass
```

### AWS Integration Testing
```python
# tests/test_aws_integration.py
import pytest
import boto3
from moto import mock_s3, mock_sagemaker
from scripts.aws.s3_manager import S3DataManager

@mock_s3
class TestS3Integration:
    """Test S3 integration with mocked AWS services."""
    
    def setup_method(self):
        """Setup mocked S3 environment."""
        self.s3_client = boto3.client('s3', region_name='us-east-1')
        self.bucket_name = 'test-thai-ocr-bucket'
        self.s3_client.create_bucket(Bucket=self.bucket_name)
        self.s3_manager = S3DataManager(self.bucket_name)
    
    def test_upload_dataset(self):
        """Test dataset upload to S3."""
        # Create temporary dataset
        with tempfile.TemporaryDirectory() as temp_dir:
            test_file = Path(temp_dir) / "test.txt"
            test_file.write_text("test data")
            
            result = self.s3_manager.upload_dataset(Path(temp_dir), "test-prefix")
            assert result is True
    
    def test_download_dataset(self):
        """Test dataset download from S3."""
        # Test download functionality
        pass

@mock_sagemaker
class TestSageMakerIntegration:
    """Test SageMaker integration."""
    
    def test_training_job_creation(self):
        """Test SageMaker training job creation."""
        # Test training job configuration
        pass
```

## Performance Testing

### Model Performance Benchmarks
```python
# tests/test_model_performance.py
import time
import pytest
from pathlib import Path
from thai_ocr.inference import ThaiOCRPredictor

class TestModelPerformance:
    """Test model performance and benchmarks."""
    
    def setup_method(self):
        """Setup model for testing."""
        model_path = "models/thai_rec_model"
        if Path(model_path).exists():
            self.predictor = ThaiOCRPredictor(model_path)
        else:
            pytest.skip("Model not available for testing")
    
    def test_inference_speed(self):
        """Test model inference speed."""
        # Load test image
        test_image = self.load_test_image()
        
        # Measure inference time
        start_time = time.time()
        result = self.predictor.predict(test_image)
        inference_time = time.time() - start_time
        
        # Assert performance requirements
        assert inference_time < 1.0, f"Inference too slow: {inference_time}s"
        assert result['confidence'] > 0.5, "Low confidence prediction"
    
    def test_batch_inference_performance(self):
        """Test batch inference performance."""
        batch_size = 10
        test_images = [self.load_test_image() for _ in range(batch_size)]
        
        start_time = time.time()
        results = [self.predictor.predict(img) for img in test_images]
        total_time = time.time() - start_time
        
        avg_time_per_image = total_time / batch_size
        assert avg_time_per_image < 0.5, f"Batch inference too slow: {avg_time_per_image}s per image"
    
    def load_test_image(self):
        """Load test image for performance testing."""
        # Implementation to load test image
        pass
```

### Load Testing
```python
# tests/test_load_testing.py
import concurrent.futures
import requests
import time
from typing import List, Dict

class TestLoadTesting:
    """Load testing for inference endpoints."""
    
    def test_concurrent_requests(self):
        """Test concurrent request handling."""
        endpoint_url = "http://localhost:8080/invocations"
        num_concurrent = 10
        num_requests_per_thread = 5
        
        def make_requests(thread_id: int) -> List[Dict]:
            """Make multiple requests from a single thread."""
            results = []
            for i in range(num_requests_per_thread):
                try:
                    response = requests.post(
                        endpoint_url,
                        files={'image': self.get_test_image()},
                        timeout=30
                    )
                    results.append({
                        'thread_id': thread_id,
                        'request_id': i,
                        'status_code': response.status_code,
                        'response_time': response.elapsed.total_seconds()
                    })
                except Exception as e:
                    results.append({
                        'thread_id': thread_id,
                        'request_id': i,
                        'error': str(e)
                    })
            return results
        
        # Execute concurrent requests
        with concurrent.futures.ThreadPoolExecutor(max_workers=num_concurrent) as executor:
            futures = [executor.submit(make_requests, i) for i in range(num_concurrent)]
            all_results = []
            for future in concurrent.futures.as_completed(futures):
                all_results.extend(future.result())
        
        # Analyze results
        successful_requests = [r for r in all_results if 'error' not in r and r['status_code'] == 200]
        success_rate = len(successful_requests) / len(all_results)
        avg_response_time = sum(r['response_time'] for r in successful_requests) / len(successful_requests)
        
        assert success_rate > 0.95, f"Success rate too low: {success_rate}"
        assert avg_response_time < 2.0, f"Average response time too high: {avg_response_time}s"
```

## Data Quality Assurance

### Dataset Validation
```python
# tests/test_data_quality.py
import pytest
from pathlib import Path
from thai_letters.data_validator import DatasetValidator

class TestDataQuality:
    """Test data quality and validation."""
    
    def setup_method(self):
        """Setup data validator."""
        self.validator = DatasetValidator()
    
    def test_image_quality(self):
        """Test image quality metrics."""
        dataset_path = Path("train_data_thai_paddleocr_v1")
        if not dataset_path.exists():
            pytest.skip("Dataset not available")
        
        quality_report = self.validator.validate_images(dataset_path / "images")
        
        # Check quality metrics
        assert quality_report['avg_resolution'][0] >= 64, "Images too small"
        assert quality_report['avg_resolution'][1] >= 256, "Images too narrow"
        assert quality_report['blur_score'] < 0.3, "Too many blurry images"
    
    def test_label_consistency(self):
        """Test label consistency and format."""
        dataset_path = Path("train_data_thai_paddleocr_v1")
        if not dataset_path.exists():
            pytest.skip("Dataset not available")
        
        consistency_report = self.validator.validate_labels(dataset_path / "train_list.txt")
        
        assert consistency_report['invalid_characters'] == 0, "Invalid characters found"
        assert consistency_report['empty_labels'] == 0, "Empty labels found"
        assert consistency_report['avg_length'] > 2, "Labels too short on average"
    
    def test_dataset_balance(self):
        """Test dataset balance and distribution."""
        dataset_path = Path("train_data_thai_paddleocr_v1")
        if not dataset_path.exists():
            pytest.skip("Dataset not available")
        
        balance_report = self.validator.analyze_distribution(dataset_path)
        
        # Check character distribution
        assert balance_report['character_coverage'] > 0.8, "Poor character coverage"
        assert balance_report['length_variance'] < 2.0, "High length variance"
```

## Continuous Integration Rules

### Pre-commit Hooks
```python
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.4.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files
        args: ['--maxkb=1000']
  
  - repo: https://github.com/psf/black
    rev: 22.10.0
    hooks:
      - id: black
        language_version: python3.8
  
  - repo: https://github.com/pycqa/flake8
    rev: 5.0.4
    hooks:
      - id: flake8
        args: [--max-line-length=88, --ignore=E203,W503]
  
  - repo: https://github.com/pycqa/isort
    rev: 5.10.1
    hooks:
      - id: isort
        args: ["--profile", "black"]
```

### Test Coverage Requirements
```python
# pytest.ini
[tool:pytest]
minversion = 6.0
addopts = 
    --cov=thai_letters
    --cov=scripts
    --cov-report=html
    --cov-report=term-missing
    --cov-fail-under=80
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
```

## Quality Metrics and Monitoring

### Model Quality Metrics
```python
# scripts/monitoring/quality_metrics.py
import logging
from typing import Dict, List, Tuple
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

class ModelQualityMonitor:
    """Monitor model quality metrics."""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def calculate_ocr_metrics(
        self, 
        predictions: List[str], 
        ground_truth: List[str]
    ) -> Dict[str, float]:
        """Calculate OCR-specific quality metrics."""
        
        # Character-level accuracy
        char_accuracy = self.character_accuracy(predictions, ground_truth)
        
        # Word-level accuracy
        word_accuracy = self.word_accuracy(predictions, ground_truth)
        
        # Edit distance metrics
        avg_edit_distance = self.average_edit_distance(predictions, ground_truth)
        
        return {
            'character_accuracy': char_accuracy,
            'word_accuracy': word_accuracy,
            'average_edit_distance': avg_edit_distance,
            'normalized_edit_distance': avg_edit_distance / max(len(p) for p in ground_truth)
        }
    
    def character_accuracy(self, predictions: List[str], ground_truth: List[str]) -> float:
        """Calculate character-level accuracy."""
        total_chars = 0
        correct_chars = 0
        
        for pred, truth in zip(predictions, ground_truth):
            total_chars += len(truth)
            for p_char, t_char in zip(pred, truth):
                if p_char == t_char:
                    correct_chars += 1
        
        return correct_chars / total_chars if total_chars > 0 else 0.0
    
    def word_accuracy(self, predictions: List[str], ground_truth: List[str]) -> float:
        """Calculate word-level accuracy."""
        correct_words = sum(1 for pred, truth in zip(predictions, ground_truth) if pred == truth)
        return correct_words / len(ground_truth) if ground_truth else 0.0
    
    def average_edit_distance(self, predictions: List[str], ground_truth: List[str]) -> float:
        """Calculate average edit distance."""
        from Levenshtein import distance
        distances = [distance(pred, truth) for pred, truth in zip(predictions, ground_truth)]
        return np.mean(distances)
```
