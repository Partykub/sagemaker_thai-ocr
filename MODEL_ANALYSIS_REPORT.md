# ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• Thai OCR

**‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà**: 1 ‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏° 2025 (‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó: 4 ‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏° 2025)  
**‡πÇ‡∏°‡πÄ‡∏î‡∏•**: SageMaker Trained Thai OCR Model  
**‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞**: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô  

---

## üöÄ **‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏ô SageMaker**

### **Overview ‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤**
‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡∏ô‡∏µ‡πâ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á end-to-end pipeline ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ó‡∏£‡∏ô Thai OCR model ‡∏ö‡∏ô AWS SageMaker ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ PaddleOCR framework

---

### **üõ†Ô∏è Infrastructure & Dependencies**

#### **AWS Services ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ**
```yaml
- AWS SageMaker: Training Jobs & Model Hosting
- AWS S3: Data Storage & Model Artifacts
- AWS ECR: Docker Container Registry  
- AWS IAM: Permission Management
- AWS CloudWatch: Monitoring & Logging
```

#### **Core Dependencies**
```bash
# Python Environment
Python 3.8+
pip install -r requirements.txt

# Key Libraries
- PaddlePaddle >= 2.5.2
- PaddleOCR >= 2.7.0
- opencv-python >= 4.5.0
- Pillow >= 8.0.0
- numpy >= 1.21.0
- boto3 >= 1.26.0

# Docker Environment
- Ubuntu 20.04 base image
- CUDA support for GPU training
- Custom PaddleOCR installation
```

#### **Project Structure**
```
sagemaker_thai-ocr/
‚îú‚îÄ‚îÄ thai-letters/              # Data generation scripts
‚îÇ   ‚îú‚îÄ‚îÄ quick_phase1_generator.py
‚îÇ   ‚îú‚îÄ‚îÄ datasets/converted/    # Converted PaddleOCR format
‚îÇ   ‚îî‚îÄ‚îÄ th_dict.txt           # Thai character dictionary
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ ml/sagemaker_trainer.py    # Main training orchestrator
‚îÇ   ‚îú‚îÄ‚îÄ continue_deployment_v2.py  # Deployment automation
‚îÇ   ‚îî‚îÄ‚îÄ training/setup_training_config.py
‚îú‚îÄ‚îÄ configs/rec/              # PaddleOCR configurations
‚îú‚îÄ‚îÄ models/sagemaker_trained/ # Downloaded trained models
‚îú‚îÄ‚îÄ Dockerfile.sagemaker      # SageMaker container definition
‚îî‚îÄ‚îÄ terraform/               # Infrastructure as Code
```

---

### **üìã Step-by-Step Training Process**

#### **Phase 1: Data Preparation**
```bash
# 1. Generate synthetic Thai data
python thai-letters/quick_phase1_generator.py

# 2. Convert to PaddleOCR format
python thai-letters/convert_to_paddleocr.py

# 3. Upload to S3
aws s3 sync ./thai-letters/datasets/converted/ s3://paddleocr-thai-data/
```

**Data Structure Created:**
```
train_data_thai_paddleocr_0731_1604/
‚îú‚îÄ‚îÄ train_data/rec/
‚îÇ   ‚îú‚îÄ‚îÄ rec_gt_train.txt      # Training labels
‚îÇ   ‚îú‚îÄ‚îÄ rec_gt_val.txt        # Validation labels
‚îÇ   ‚îî‚îÄ‚îÄ thai_data/
‚îÇ       ‚îú‚îÄ‚îÄ train/            # 6,106 training images
‚îÇ       ‚îî‚îÄ‚îÄ val/              # 878 validation images
‚îî‚îÄ‚îÄ th_dict.txt              # 880 Thai characters
```

#### **Phase 2: Infrastructure Setup**
```bash
# 1. Setup Terraform infrastructure
cd terraform/
terraform init
terraform plan
terraform apply

# 2. Build and push Docker container
docker build -f Dockerfile.sagemaker -t paddleocr-thai .
aws ecr get-login-password | docker login --username AWS --password-stdin
docker tag paddleocr-thai:latest 484468818942.dkr.ecr.ap-southeast-1.amazonaws.com/paddleocr-thai
docker push 484468818942.dkr.ecr.ap-southeast-1.amazonaws.com/paddleocr-thai
```

**Infrastructure Created:**
- **S3 Bucket**: `paddleocr-thai-data` (data storage)
- **ECR Repository**: `paddleocr-thai` (container images)
- **IAM Role**: `paddleocr-sagemaker-role` (permissions)
- **SageMaker Training Job**: Auto-configured

#### **Phase 3: Model Configuration**
```yaml
# configs/rec/thai_rec_sagemaker.yml
Global:
  epoch_num: 100
  character_dict_path: /opt/ml/input/data/training/th_dict.txt
  character_type: thai
  max_text_length: 25
  save_model_dir: /opt/ml/model/

Architecture:
  model_type: rec
  algorithm: CRNN
  Backbone:
    name: MobileNetV3
    scale: 0.5
  Neck:
    name: SequenceEncoder
    encoder_type: rnn
    hidden_size: 96
  Head:
    name: CTCHead

Optimizer:
  name: Adam
  lr:
    learning_rate: 0.001
    name: Cosine
    warmup_epoch: 5
```

#### **Phase 4: Training Execution**
```bash
# 1. Start SageMaker training job
python scripts/ml/sagemaker_trainer.py \
  --config configs/rec/thai_rec_sagemaker.yml \
  --data-path s3://paddleocr-thai-data/train_data_thai_paddleocr_0731_1604/ \
  --output-path s3://paddleocr-thai-models/

# 2. Monitor training progress
aws sagemaker describe-training-job --training-job-name paddleocr-thai-training-1753958543

# 3. Download trained model
python scripts/continue_deployment_v2.py
```

**Training Results:**
- **Duration**: ~45 minutes (100 epochs)
- **Instance Type**: ml.m5.xlarge (CPU training)
- **Model Size**: 8.8MB final weights
- **Training Status**: Completed successfully

#### **Phase 5: Model Deployment & Testing**
```bash
# 1. Download model artifacts
aws s3 sync s3://paddleocr-thai-models/output/ ./models/sagemaker_trained/

# 2. Setup local inference config
cp models/sagemaker_trained/config.yml models/sagemaker_trained/config_local.yml
# Edit paths for local environment

# 3. Test trained model
python scripts/ml/test_trained_model_correct_config.py
```

---

### **üîß Key Technical Configurations**

#### **Docker Container Setup**
```dockerfile
FROM python:3.8-slim

# Install system dependencies
RUN apt-get update && apt-get install -y \
    libglib2.0-0 libsm6 libxext6 libxrender-dev libgomp1 \
    git wget curl && rm -rf /var/lib/apt/lists/*

# Install PaddlePaddle and PaddleOCR
RUN pip install paddlepaddle==2.5.2
RUN pip install paddleocr==2.7.0

# Copy training scripts
COPY scripts/ /opt/ml/code/scripts/
COPY configs/ /opt/ml/code/configs/

# Set entrypoint for SageMaker
ENV PYTHONPATH=/opt/ml/code
ENTRYPOINT ["python", "/opt/ml/code/scripts/training/train.py"]
```

#### **SageMaker Training Job Configuration**
```python
training_job_config = {
    'TrainingJobName': f'paddleocr-thai-training-{int(time.time())}',
    'AlgorithmSpecification': {
        'TrainingImage': '484468818942.dkr.ecr.ap-southeast-1.amazonaws.com/paddleocr-thai:latest',
        'TrainingInputMode': 'File'
    },
    'RoleArn': 'arn:aws:iam::484468818942:role/paddleocr-sagemaker-role',
    'InputDataConfig': [{
        'ChannelName': 'training',
        'DataSource': {
            'S3DataSource': {
                'S3DataType': 'S3Prefix',
                'S3Uri': 's3://paddleocr-thai-data/train_data_thai_paddleocr_0731_1604/',
                'S3DataDistributionType': 'FullyReplicated'
            }
        }
    }],
    'OutputDataConfig': {
        'S3OutputPath': 's3://paddleocr-thai-models/'
    },
    'ResourceConfig': {
        'InstanceType': 'ml.m5.xlarge',
        'InstanceCount': 1,
        'VolumeSizeInGB': 20
    },
    'StoppingCondition': {
        'MaxRuntimeInSeconds': 7200  # 2 hours max
    }
}
```

#### **IAM Permissions Required**
```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:GetObject",
                "s3:PutObject",
                "s3:ListBucket"
            ],
            "Resource": [
                "arn:aws:s3:::paddleocr-thai-*",
                "arn:aws:s3:::paddleocr-thai-*/*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "ecr:GetAuthorizationToken",
                "ecr:BatchCheckLayerAvailability",
                "ecr:GetDownloadUrlForLayer",
                "ecr:BatchGetImage"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "logs:CreateLogGroup",
                "logs:CreateLogStream",
                "logs:PutLogEvents"
            ],
            "Resource": "arn:aws:logs:*:*:*"
        }
    ]
}
```

---

### **üìä Training Statistics & Metrics**

#### **Dataset Composition**
- **Total Images**: 6,984 images
- **Training Set**: 6,106 images (87.4%)
- **Validation Set**: 878 images (12.6%)
- **Character Classes**: 880 unique Thai characters
- **Image Size**: 32x100 pixels (standardized)

#### **Training Performance**
```
Epochs: 100
Learning Rate: 0.001 (Cosine schedule)
Batch Size: 128
Training Time: ~45 minutes
Final Loss: Converged successfully
Model Size: 8.8MB
```

#### **Infrastructure Costs**
```
SageMaker Training: ~$2.50 (ml.m5.xlarge √ó 45 min)
S3 Storage: ~$0.10/month (1GB data)
ECR Storage: ~$0.10/month (500MB container)
Total: ~$2.70 one-time training cost
```

---

### **üéØ Success Factors**

#### **What Worked Well**
1. **Automated Pipeline**: End-to-end automation from data to model
2. **Scalable Infrastructure**: Terraform-managed AWS resources
3. **Container-based Training**: Consistent environment across local/cloud
4. **Data Pipeline**: Efficient PaddleOCR format conversion
5. **Model Artifacts**: Proper model saving and downloading

#### **Key Learnings**
1. **PaddleOCR Integration**: Successfully adapted for Thai language
2. **SageMaker Best Practices**: Proper job configuration and monitoring
3. **Container Optimization**: Efficient Docker image for training
4. **Data Quality**: Importance of proper data formatting
5. **Configuration Management**: Flexible config system for different environments

---

---

## üìö **Technical References & Commands**

### **Useful Commands for Development**
```bash
# Check training job status
aws sagemaker describe-training-job --training-job-name paddleocr-thai-training-1753958543

# Monitor CloudWatch logs
aws logs tail /aws/sagemaker/TrainingJobs --follow --start-time -10m

# Download model artifacts
aws s3 sync s3://paddleocr-thai-models/paddleocr-thai-training-1753958543/output/ ./models/sagemaker_trained/

# Test local inference
python scripts/ml/test_trained_model_correct_config.py

# Check model files
ls -la models/sagemaker_trained/best_model/
# model.pdparams (8.8MB), model.pdopt, model.states
```

### **Configuration Files Reference**
- **Training Config**: `configs/rec/thai_rec_sagemaker.yml`
- **Local Inference Config**: `models/sagemaker_trained/config_local.yml`
- **Docker Definition**: `Dockerfile.sagemaker`
- **Infrastructure**: `terraform/main.tf`
- **Character Dictionary**: `thai-letters/th_dict.txt`

### **Key Scripts Developed**
- **`scripts/ml/sagemaker_trainer.py`**: Main training orchestrator
- **`scripts/continue_deployment_v2.py`**: Model deployment automation
- **`scripts/training/setup_training_config.py`**: Configuration setup
- **`scripts/ml/test_trained_model_correct_config.py`**: Model testing
- **`thai-letters/quick_phase1_generator.py`**: Data generation

---

## üîç **Troubleshooting & Common Issues**

### **Data-related Issues**
```bash
# Fix dictionary path mismatch
sed -i 's|/opt/ml/input/data/training/|./thai-letters/|g' models/sagemaker_trained/config_local.yml

# Verify data format
head -5 thai-letters/datasets/converted/train_data_thai_paddleocr_0731_1604/train_data/rec/rec_gt_train.txt

# Check image files
ls thai-letters/datasets/converted/train_data_thai_paddleocr_0731_1604/train_data/rec/thai_data/train/ | head -10
```

### **Model Loading Issues**
```python
# Test model loading
import paddle
model_path = "models/sagemaker_trained/best_model/model.pdparams"
state_dict = paddle.load(model_path)
print(f"Model parameters loaded: {len(state_dict)} layers")
```

### **SageMaker Debugging**
```bash
# Check training job logs
aws logs describe-log-groups --log-group-name-prefix /aws/sagemaker/TrainingJobs

# Download all artifacts
aws s3 cp s3://paddleocr-thai-models/paddleocr-thai-training-1753958543/ ./debug/ --recursive

# Check container status
docker run --rm paddleocr-thai:latest python -c "import paddleocr; print('PaddleOCR loaded successfully')"
```

---

## üéØ **‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö**

### ‚úÖ **‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à**
- **‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ**: ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô trained model ‡∏à‡∏≤‡∏Å SageMaker ‡πÑ‡∏î‡πâ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
- **Pipeline ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô**: ‡∏Å‡∏≤‡∏£ inference ‡∏ú‡πà‡∏≤‡∏ô PaddleOCR tools ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ
- **‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢**: ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÑ‡∏î‡πâ
- **Configuration**: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç path ‡πÅ‡∏•‡∏∞ dictionary ‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ

### ‚ùå **‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏ö**
- **Accuracy: 0%** ‡∏à‡∏≤‡∏Å 10 ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏î‡∏™‡∏≠‡∏ö
- **Output ‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ**: ‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á 1-3 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ ‡πÅ‡∏ï‡πà‡πÑ‡∏î‡πâ 20-30 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£
- **‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î**: ‡∏°‡∏µ‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏õ‡∏ô‡∏°‡∏≤

---

## üìä **‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö**

### **‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå**

| ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û | Ground Truth | Predicted | ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á |
|--------|--------------|-----------|-------------|
| 772_00.jpg | `‡∏´‡πá` | `‡∏ó‡∏π‡πâ‡∏ë‡∏±‡∏á‡∏∏‡πà‡∏ç‡∏µ‡πà‡∏á‡∏∏‡πà‡∏ï‡∏∑‡∏ç‡∏µ‡πà‡∏á‡∏∏‡πà‡∏ó‡∏µ‡πàP‡πÄ‡∏á‡∏∏‡πà‡∏ç‡∏µ‡πà‡∏á‡∏∏‡πà6‡∏á‡∏∏‡πà` | ‚ùå |
| 820_03.jpg | `‡∏Å‡∏∏` | `‡∏ô‡∏¥‡πâ‡∏ï‡πã]‡∏è‡πå]‡∏à‡∏µ‡∏¢‡∏¥‡πâ]‡∏†‡πâ‡∏û‡∏¢‡πà‡∏´‡∏¥‡πâ‡∏™‡∏µ‡πâ‡∏¢‡πà` | ‚ùå |
| 299_02.jpg | `‡∏©` | `‡∏ä‡πç‡∏°‡∏π‡πà` | ‚ùå |
| 321_03.jpg | `‡∏´‡∏µ‡πà` | `‡∏õ‡∏∑‡πâ‡∏î‡πç‡∏á‡∏∑‡∏ô‡∏µ‡πâ` | ‚ùå |
| 599_04.jpg | `‡∏ß‡∏π` | `‡∏à‡∏±‡πâ‡∏™‡∏¥‡∏à‡∏∑‡πà‡∏ï‡πá‡∏†‡∏∏‡∏£‡∏≥‡∏Ç‡∏∑‡πà‡∏ï‡πà‡∏≥‡∏•‡πå‡∏ú‡∏∏‡πâ‡∏£‡∏≥‡∏≠` | ‚ùå |

### **Confidence Scores**
- ‡∏£‡∏∞‡∏î‡∏±‡∏ö confidence ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà ~0.0014-0.0015 (‡∏ï‡πà‡∏≥‡∏°‡∏≤‡∏Å)
- ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢

---

## üîç **‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏õ‡∏±‡∏ç‡∏´‡∏≤**

### **1. Overfitting / Hallucination**
```
Expected: "‡∏´‡πá" (1 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£)
Got: "‡∏ó‡∏π‡πâ‡∏ë‡∏±‡∏á‡∏∏‡πà‡∏ç‡∏µ‡πà‡∏á‡∏∏‡πà‡∏ï‡∏∑‡∏ç‡∏µ‡πà‡∏á‡∏∏‡πà‡∏ó‡∏µ‡πàP‡πÄ‡∏á‡∏∏‡πà‡∏ç‡∏µ‡πà‡∏á‡∏∏‡πà6‡∏á‡∏∏‡πà" (30+ ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£)
```

**‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ**:
- `max_text_length: 25` ‡∏™‡∏π‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö single character
- CTC decoding ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏¢‡∏≤‡∏ß‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
- Training data ‡∏≠‡∏≤‡∏à‡∏°‡∏µ multi-character samples ‡∏õ‡∏ô‡∏≠‡∏¢‡∏π‡πà

### **2. ‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÅ‡∏õ‡∏•‡∏Å‡πÜ ‡∏õ‡∏ô‡∏°‡∏≤**
**‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ**: `P`, `6`, `]`, `&`, `f`, `H`

**‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ**:
- Dictionary ‡∏°‡∏µ‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ
- Model architecture ‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö single character recognition
- Training process ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤

### **3. Low Confidence**
**Score ‡∏ï‡πà‡∏≥ (~0.001)** ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤:
- ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
- Feature extraction ‡πÑ‡∏°‡πà‡∏î‡∏µ‡∏û‡∏≠
- Training convergence ‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå

---

## üõ†Ô∏è **‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç**

### **Phase 1: ‡∏õ‡∏£‡∏±‡∏ö Configuration (‡πÄ‡∏£‡πá‡∏ß)**

#### **1.1 ‡∏õ‡∏£‡∏±‡∏ö Inference Parameters**
```yaml
Global:
  max_text_length: 1      # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 25 ‡πÄ‡∏õ‡πá‡∏ô 1
  infer_mode: true        # ‡πÄ‡∏õ‡∏¥‡∏î inference mode
  use_space_char: false   # ‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ space
```

#### **1.2 ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Dictionary**
```bash
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÉ‡∏ô th_dict.txt
head -20 thai-letters/th_dict.txt
wc -l thai-letters/th_dict.txt  # ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô ~25 ‡∏ï‡∏±‡∏ß ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà 880
```

#### **1.3 ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö‡∏£‡∏π‡∏õ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**
```bash
# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö systematic
python scripts/ml/systematic_model_test.py
```

### **Phase 2: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Data Quality (‡∏Å‡∏•‡∏≤‡∏á)**

#### **2.1 ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Training Data**
```bash
# ‡∏î‡∏π label format
head -20 thai-letters/datasets/.../rec_gt_train.txt
```

#### **2.2 ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ Character Distribution**
```python
# ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á
python scripts/ml/analyze_character_distribution.py
```

### **Phase 3: Re-training (‡∏ä‡πâ‡∏≤)**

#### **3.1 ‡∏õ‡∏£‡∏±‡∏ö Training Config**
```yaml
Global:
  max_text_length: 3      # ‡∏•‡∏î‡∏•‡∏á
  epoch_num: 50           # ‡∏•‡∏î epochs
  
Optimizer:
  lr:
    learning_rate: 0.0001 # ‡∏•‡∏î learning rate
```

#### **3.2 ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Architecture**
```yaml
Architecture:
  algorithm: SimpleRec    # ‡πÉ‡∏ä‡πâ simple classifier ‡πÅ‡∏ó‡∏ô CRNN
  # ‡∏´‡∏£‡∏∑‡∏≠
  algorithm: SVTR_Tiny    # ‡πÉ‡∏ä‡πâ lightweight model
```

---

## üìà **‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠**

### **‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏ó‡∏µ‡πà 1: Quick Fixes**
- [ ] ‡∏õ‡∏£‡∏±‡∏ö `max_text_length = 1`
- [ ] ‡∏ó‡∏î‡∏™‡∏≠‡∏ö 50 ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
- [ ] ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå dictionary content
- [ ] ‡∏õ‡∏£‡∏±‡∏ö CTC decoding parameters

### **‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏ó‡∏µ‡πà 2: Data Analysis**
- [ ] ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö training data quality
- [ ] ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå character distribution
- [ ] ‡∏´‡∏≤ root cause ‡∏Ç‡∏≠‡∏á‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÅ‡∏õ‡∏•‡∏Å
- [ ] ‡∏™‡∏£‡πâ‡∏≤‡∏á clean dataset

### **‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏ó‡∏µ‡πà 3: Model Improvement**
- [ ] Re-train ‡∏Å‡∏±‡∏ö config ‡πÉ‡∏´‡∏°‡πà
- [ ] ‡∏ó‡∏î‡∏™‡∏≠‡∏ö architecture ‡∏ï‡πà‡∏≤‡∏á‡πÜ
- [ ] Hyperparameter tuning
- [ ] Final evaluation

---

## üéØ **‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á**

### **Short-term (1 ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå)**
- **Accuracy > 10%** ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö config
- **Output length ‚â§ 5 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£**
- **‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÅ‡∏õ‡∏•‡∏Å** ‡πÉ‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå

### **Medium-term (2-3 ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå)**
- **Accuracy > 50%** single character recognition
- **Confidence > 0.8** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å
- **Clean output** ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢

### **Long-term (1 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô)**
- **Accuracy > 80%** Thai character recognition
- **Support multi-character** 2-3 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£
- **Production ready** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á

---

## üìù **‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç**

### **Technical Details**
- **Model Architecture**: CRNN + MobileNetV3 + CTCHead
- **Model Size**: 8.8MB (trained weights)
- **Dictionary Size**: 880 characters (‡∏Ñ‡∏ß‡∏£‡∏•‡∏î‡πÄ‡∏õ‡πá‡∏ô ~25)
- **Training Epochs**: 100 (‡∏≠‡∏≤‡∏à‡πÄ‡∏¢‡∏≠‡∏∞‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ)

### **Infrastructure**
- **SageMaker Training**: ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ
- **Model Download**: ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
- **Local Inference**: ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ

### **Next Steps Priority**
1. **üî• ‡∏™‡∏π‡∏á**: ‡∏õ‡∏£‡∏±‡∏ö `max_text_length` ‡πÅ‡∏•‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö
2. **üî• ‡∏™‡∏π‡∏á**: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö dictionary content
3. **‚ö° ‡∏Å‡∏•‡∏≤‡∏á**: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå training data
4. **üìÖ ‡∏ï‡πà‡∏≥**: Re-training ‡∏Å‡∏±‡∏ö config ‡πÉ‡∏´‡∏°‡πà

---

## üìö **‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á**

- **Config Files**: `models/sagemaker_trained/config_local.yml`
- **Test Results**: `TRAINED_MODEL_CORRECT_CONFIG_RESULTS_20250801_164536.json`
- **Test Script**: `scripts/ml/test_trained_model_correct_config.py`
- **Model Weights**: `models/sagemaker_trained/best_model/model.pdparams`

---

## üìù **‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç**

### **Technical Architecture Summary**
```
Data Flow: Thai Images ‚Üí PaddleOCR Format ‚Üí S3 ‚Üí SageMaker ‚Üí Trained Model ‚Üí Local Testing

Technology Stack:
‚îú‚îÄ‚îÄ Data Generation: Python + PIL + Thai Fonts
‚îú‚îÄ‚îÄ Data Format: PaddleOCR compatible structure
‚îú‚îÄ‚îÄ Training: AWS SageMaker + PaddlePaddle + CRNN
‚îú‚îÄ‚îÄ Storage: AWS S3 + ECR
‚îú‚îÄ‚îÄ Infrastructure: Terraform + AWS IAM
‚îî‚îÄ‚îÄ Testing: Local PaddleOCR inference
```

### **Model Architecture Details**
- **Backbone**: MobileNetV3 (scale: 0.5) - Lightweight CNN
- **Neck**: SequenceEncoder (RNN, hidden_size: 96) - Sequence modeling
- **Head**: CTCHead - Connectionist Temporal Classification
- **Input Size**: [3, 32, 100] - 3 channels, 32px height, 100px width
- **Output**: Variable length Thai character sequences

### **Success Metrics**
‚úÖ **Infrastructure**: 100% automated deployment  
‚úÖ **Data Pipeline**: 6,984 Thai images processed  
‚úÖ **Training**: 100 epochs completed successfully  
‚úÖ **Model Export**: 8.8MB trained weights extracted  
‚úÖ **Local Inference**: Model loading and prediction working  

### **Future Improvements Identified**
- **Model Quality**: Need accuracy optimization (currently 0%)
- **Output Length**: Reduce from 25+ to 1-3 characters  
- **Character Set**: Clean dictionary to remove non-Thai characters
- **Confidence Scores**: Improve from ~0.001 to >0.8
- **Architecture**: Consider lighter models for single characters

### **Resource Usage**
- **Development Time**: ~3 days end-to-end
- **AWS Costs**: ~$2.70 total training cost
- **Data Size**: 1GB training dataset
- **Model Size**: 8.8MB production model
- **Training Time**: 45 minutes on ml.m5.xlarge

---

*‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: 4 ‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏° 2025*  
*‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà data generation ‡∏à‡∏ô‡∏ñ‡∏∂‡∏á model testing*
