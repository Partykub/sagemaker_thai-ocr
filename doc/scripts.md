# Scripts Documentation

This document outlines all scripts in the Thai OCR project, their purposes, usage, and when to use them.

## Overview

The `scripts/` directory contains automation and management scripts for the Thai OCR project. Scripts are organized by functionality and follow consistent naming conventions.

## üéØ Recent Updates (August 11, 2025)

### üéâ **‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ**

#### **‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô (Training Files)**

**Model Files ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á**:
```
models/sagemaker_trained/
‚îú‚îÄ‚îÄ best_accuracy.pdparams     # ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏•‡∏±‡∏Å (9,205,880 bytes)
‚îú‚îÄ‚îÄ best_accuracy.pdopt        # Optimizer state
‚îú‚îÄ‚îÄ config.yml                 # Training configuration (2,262 bytes)
‚îî‚îÄ‚îÄ best_model/
    ‚îú‚îÄ‚îÄ model.pdparams         # Alternative model format
    ‚îî‚îÄ‚îÄ model.pdopt            # Alternative optimizer
```

**Dictionary Files ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Character Set**:
```
thai-letters/
‚îú‚îÄ‚îÄ th_dict.txt               # Thai characters (880 characters, 7,323 bytes)
‚îú‚îÄ‚îÄ numbers_dict.txt          # Numbers 0-9 only
‚îî‚îÄ‚îÄ number_dict.txt           # Alternative numbers dictionary
```

**Configuration Files ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô**:
```
configs/rec/
‚îú‚îÄ‚îÄ thai_rec_dev.yml          # Development training (10 epochs)
‚îú‚îÄ‚îÄ thai_rec.yml              # Standard training (100 epochs)
‚îú‚îÄ‚îÄ thai_rec_prod.yml         # Production training (200 epochs)
‚îú‚îÄ‚îÄ numbers_inference_config.yml  # Numbers model inference
‚îî‚îÄ‚îÄ quick_single_char_config.yml  # Single character training
```

**Training Data Structure**:
```
thai-letters/datasets/converted/train_data_thai_paddleocr_*/
‚îú‚îÄ‚îÄ train_data/rec/
‚îÇ   ‚îú‚îÄ‚îÄ rec_gt_train.txt      # Training labels (image_path\tground_truth)
‚îÇ   ‚îú‚îÄ‚îÄ rec_gt_val.txt        # Validation labels (15 samples)
‚îÇ   ‚îî‚îÄ‚îÄ thai_data/
‚îÇ       ‚îú‚îÄ‚îÄ train/            # Training images
‚îÇ       ‚îî‚îÄ‚îÄ val/              # Validation images
```

#### `test_numbers_model.py` - **SUCCESSFUL CUSTOM MODEL TESTING**
**Purpose**: Complete validation of SageMaker-trained numbers model with proper dataset

**Description**: 
- ‚úÖ **Model Integration Success**: Uses custom trained model (best_accuracy.pdparams)
- ‚úÖ **Real Inference Testing**: 100% inference success rate (15/15 samples)
- ‚úÖ **Proper Architecture Match**: CRNN + MobileNetV3 consistency
- ‚úÖ **Validation Dataset**: Uses correct numbers dataset (0-9)
- üìä **Performance Metrics**: 13.3% accuracy, identified improvement areas

**‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô**:
```bash
# Model file - ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏•‡πâ‡∏ß
models/sagemaker_trained/best_accuracy.pdparams

# Dictionary file - ‡∏ä‡∏∏‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö
thai-letters/th_dict.txt  # ‡∏´‡∏£‡∏∑‡∏≠ numbers_dict.txt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç

# Configuration file - ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô
configs/rec/thai_rec.yml  # ‡∏´‡∏£‡∏∑‡∏≠ numbers_inference_config.yml

# Training data - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ó‡∏£‡∏ô
thai-letters/datasets/converted/train_data_thai_paddleocr_*/train_data/rec/
```

**Usage**:
```bash
# Complete model testing with validation data
python test_numbers_model.py

# Outputs:
# - numbers_model_test_results_YYYYMMDD_HHMMSS.json
# - numbers_dict.txt
# - numbers_inference_config.yml
```

**Key Features**:
- ‚úÖ **Custom Model Loading**: Uses SageMaker trained model weights
- ‚úÖ **Architecture Verification**: CRNN + MobileNetV3 + CTC consistency  
- ‚úÖ **Proper Dictionary**: Numbers 0-9 character set
- ‚úÖ **Batch Testing**: Automated validation with 15 samples
- ‚úÖ **Confidence Scoring**: Real model confidence outputs
- ‚úÖ **JSON Results**: Complete test results with metrics

**When to use**:
- After SageMaker training completion
- Model validation and performance assessment
- Debugging inference issues
- Architecture consistency verification

### üìö **‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô Thai OCR Model ‡πÅ‡∏ö‡∏ö‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå**

#### **‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô (Training Workflow)**

**1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Data Preparation)**:
```bash
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ó‡∏£‡∏ô
cd thai-letters
python thai_dataset_quick.py 100  # ‡∏™‡∏£‡πâ‡∏≤‡∏á 100 ‡∏†‡∏≤‡∏û

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å dictionary ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£:
# - numbers_dict.txt (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç 0-9)
# - th_dict.txt (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÑ‡∏ó‡∏¢)

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å effects:
# - 0 = ‡πÑ‡∏°‡πà‡∏°‡∏µ effects (‡∏†‡∏≤‡∏û‡∏ä‡∏±‡∏î)
# - 9 = effects ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡∏†‡∏≤‡∏û‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢)
```

**2. ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô PaddleOCR Format**:
```bash
# ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö PaddleOCR
python phase1_paddleocr_converter.py \
  --input-path thai_dataset_YYYYMMDD_HHMM/ \
  --output-path train_data_thai_paddleocr_v1/
```

**3. ‡∏™‡∏£‡πâ‡∏≤‡∏á Configuration Files**:
```bash
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå config ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ó‡∏£‡∏ô
python ../scripts/training/setup_training_config.py

# ‡πÑ‡∏î‡πâ‡πÑ‡∏ü‡∏•‡πå:
# - configs/rec/thai_rec_dev.yml (10 epochs - ‡∏ó‡∏î‡∏™‡∏≠‡∏ö)
# - configs/rec/thai_rec.yml (100 epochs - ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô)
# - configs/rec/thai_rec_prod.yml (200 epochs - production)
```

**4. ‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏ö‡∏ö Local (‡∏ó‡∏î‡∏™‡∏≠‡∏ö)**:
```bash
cd PaddleOCR
python tools/train.py -c ../configs/rec/thai_rec_dev.yml
```

**5. ‡πÄ‡∏ó‡∏£‡∏ô‡∏ö‡∏ô SageMaker (Production)**:
```bash
# ‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏ö‡∏ö‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
python scripts/continue_deployment_v2.py

# ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏ö‡∏ö manual
python scripts/training/manual_numbers_training.py
```

**6. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•**:
```bash
# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏•‡πâ‡∏ß
python test_numbers_model.py

# ‡∏´‡∏£‡∏∑‡∏≠‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö manual
cd PaddleOCR
python tools/infer_rec.py \
  -c "../test_inference_config.yml" \
  -o Global.infer_img="../test_images/image.jpg"
```

#### **‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ (Required Files)**

**‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏ó‡∏£‡∏ô**:
- `thai-letters/th_dict.txt` - Character dictionary
- `configs/rec/thai_rec.yml` - Training configuration
- `train_data_thai_paddleocr_*/train_data/rec/` - Training data

**‡∏´‡∏•‡∏±‡∏á‡πÄ‡∏ó‡∏£‡∏ô**:
- `models/sagemaker_trained/best_accuracy.pdparams` - Trained model
- `models/sagemaker_trained/config.yml` - Model configuration
- `output/rec/` - Local training outputs

**‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Inference**:
- Model file (.pdparams)
- Dictionary file (.txt)
- Configuration file (.yml)

### ‚ú® **Enhanced Dataset Generation**

#### `thai-letters/thai_dataset_quick.py` - **ENHANCED INTERACTIVE GENERATOR**
**Purpose**: Interactive Thai dataset generation with flexible dictionary and effects selection

**Description**: 
- Interactive dictionary file selection from available `*_dict.txt` files
- Flexible effects selection (8 OCR challenge types)
- Enhanced image dimensions (128x96 pixels, +50% height)
- Improved font size range (42-84 pixels)
- Smart output folder naming with effects information
- Seamless integration with `thai_dataset_generator.py`

**Usage**:
```bash
# Navigate to thai-letters folder
cd thai-letters

# Interactive dataset generation
python thai_dataset_quick.py <number_of_samples>

# Examples:
python thai_dataset_quick.py 1    # Quick test
python thai_dataset_quick.py 10   # Standard
python thai_dataset_quick.py 20   # High quality
```

**Interactive Selections**:
1. **Dictionary Selection**: Choose from available dictionary files
2. **Effects Selection**: 
   - `0`: No effects (ideal conditions)
   - `9`: All effects (recommended)
   - `1,2,3`: Specific effects (custom combinations)

**Available Effects (8 types)**:
- Rotation (-2 to +2 degrees)
- Brightness (0.8-1.2)
- Contrast (0.8-1.2)
- Blur (0-0.4)
- Noise Level (0-0.05)
- Position (center-left, center, center-right)
- Padding (15-25 pixels)
- Compression (85-100% quality)

**When to use**:
- ‚úÖ Creating training datasets with specific characteristics
- ‚úÖ Testing OCR robustness with controlled challenges
- ‚úÖ Generating clean reference images (effects=0)
- ‚úÖ Producing varied datasets for model training (effects=9)

**Key Features**:
- ‚úÖ Interactive user interface
- ‚úÖ Flexible effects combinations
- ‚úÖ Enhanced image quality (128x96 pixels)
- ‚úÖ Smart folder naming system
- ‚úÖ Parameter integration with generator
- ‚úÖ Support for multiple dictionary files

**Recent Improvements**:
- Enhanced from previous `thai_dataset_quick_v2.py`
- Fixed "no effects" functionality
- Improved parameter passing to core generator
- Better output folder naming with effects information
- Removed redundant v2 file for cleaner project structure

### ‚úÖ **Completed & Working Scripts**

#### **Model Testing (VERIFIED WORKING)**

#### `test_sagemaker_model.py` - **PRIMARY TESTING SCRIPT**
**Purpose**: Standardized testing of SageMaker-trained Thai OCR model with exact training configuration

**Description**: 
- Uses EXACT same configuration as training (CRNN + MobileNetV3)
- Tests with standardized validation dataset with ground truth labels
- Provides detailed accuracy metrics and confidence scores
- Ensures configuration consistency between training and inference
- Outputs single character results as designed

**Usage**:
```bash
# Standard model testing (RECOMMENDED)
python test_sagemaker_model.py
```

**Configuration Used**:
- **Model**: `models/sagemaker_trained/best_accuracy.pdparams` (9,205,880 bytes)
- **Dictionary**: `thai-letters/th_dict.txt` (880 characters, 7,323 bytes)
- **Architecture**: CRNN + MobileNetV3 (scale: 0.5, hidden_size: 96)
- **Max Text Length**: 1 (single character mode)
- **Test Dataset**: `rec_gt_val.txt` (15 samples with ground truth)

**When to use**:
- ‚úÖ Primary testing method for trained model validation
- ‚úÖ When verifying model performance with known ground truth
- ‚úÖ For consistent, repeatable testing results
- ‚úÖ To validate configuration compatibility

**Key Features**:
- ‚úÖ Exact training configuration match
- ‚úÖ Standardized test dataset (15 validation samples)
- ‚úÖ Ground truth comparison with accuracy metrics
- ‚úÖ JSON result export for analysis
- ‚úÖ 93.3% inference success rate verified

**Current Performance**:
- Model Loading: 100% success
- Inference Execution: 93.3% success (14/15 samples)
- Single Character Output: Working
- Character Accuracy: Low (needs improvement)

---

## üöÄ SageMaker Training Scripts

### Manual Training Scripts

#### `scripts/training/manual_numbers_training.py`
**Purpose**: Manual SageMaker training job creator for Thai Numbers OCR

**Description**: 
- Creates and monitors SageMaker training jobs manually with full control
- Uses ml.g4dn.xlarge GPU instances for efficient training
- Provides real-time training progress monitoring
- Calculates cost estimates and training metrics

**Usage**:
```bash
# Prerequisites: Set AWS credentials
$Env:AWS_ACCESS_KEY_ID="your_access_key"
$Env:AWS_SECRET_ACCESS_KEY="your_secret_key" 
$Env:AWS_SESSION_TOKEN="your_session_token"

# Start training
python scripts/training/manual_numbers_training.py
```

**When to use**:
- Manual control over training job creation
- Real-time monitoring of training progress
- Custom hyperparameter configuration
- Cost-controlled training experiments

**Key Features**:
- ‚úÖ GPU instance support (ml.g4dn.xlarge)
- ‚úÖ Real-time progress monitoring
- ‚úÖ Cost calculation and estimates ($0.11 for numbers dataset)
- ‚úÖ Automatic model artifact handling
- ‚úÖ Error handling and retry logic

#### `scripts/training/validate_training_setup.py`
**Purpose**: Pre-training validation for AWS resources and configuration

**Description**:
- Validates AWS credentials and permissions
- Checks ECR image availability and S3 data
- Verifies SageMaker IAM roles and local configurations
- Provides training cost estimates

**Usage**:
```bash
python scripts/training/validate_training_setup.py
```

**When to use**:
- Before starting any training job
- Troubleshooting AWS setup issues
- Verifying resource availability

**Key Features**:
- ‚úÖ Comprehensive AWS resource validation
- ‚úÖ Cost estimation
- ‚úÖ Clear error reporting
- ‚úÖ Setup verification checklist

#### `scripts/training/download_trained_model.py`
**Purpose**: Download and extract trained models from S3

**Description**:
- Downloads model artifacts from SageMaker training jobs
- Extracts and organizes model files locally
- Attempts basic model testing with PaddleOCR
- Creates model inventory and test results

**Usage**:
```bash
python scripts/training/download_trained_model.py
```

**When to use**:
- After successful training completion
- Local model testing and validation
- Model artifact management

**Key Features**:
- ‚úÖ Automatic S3 download and extraction
- ‚úÖ Model file organization
- ‚úÖ Basic inference testing
- ‚úÖ Result documentation

#### `scripts/training/simple_model_test.py`
**Purpose**: Simple testing and validation of downloaded models

**Description**:
- Creates comprehensive model reports
- Tests model compatibility with PaddleOCR
- Generates usage recommendations
- Documents model configuration and capabilities

**Usage**:
```bash
python scripts/training/simple_model_test.py
```

**When to use**:
- Quick model validation
- Generating model documentation
- Compatibility testing

**Key Features**:
- ‚úÖ Model compatibility checking
- ‚úÖ Configuration analysis
- ‚úÖ Usage documentation
- ‚úÖ Next steps recommendations

### Recent Training Success (August 7, 2025)
- **Job**: `thai-numbers-ocr-20250807-100059`
- **Duration**: 13 minutes (781 seconds)
- **Cost**: $0.11 USD
- **Instance**: ml.g4dn.xlarge (GPU)
- **Status**: ‚úÖ Completed successfully
- **Model**: Ready for numbers 0-9 recognition

#### `quick_single_char_test.py` - **QUICK TESTING SCRIPT**
**Purpose**: Simple, quick testing for rapid model validation

**Description**: 
- Simplified testing approach for quick verification
- Tests basic model loading and inference pipeline
- Provides immediate feedback on model functionality
- Less comprehensive than main testing script

**Usage**:
```bash
# Quick model validation
python quick_single_char_test.py
```

**When to use**:
- ‚úÖ Quick model functionality check
- ‚úÖ Basic inference pipeline validation
- ‚úÖ Development and debugging
- ‚úÖ When main testing script is too comprehensive

#### **Training & Data Generation (COMPLETED)**

#### `thai-letters/quick_phase1_generator.py` - **DATA GENERATION**
**Purpose**: Generate synthetic Thai character images for training

**Description**: 
- Successfully generated 9,408 synthetic Thai images
- Multiple fonts and styles for data diversity
- Creates ground truth labels automatically
- Optimized for PaddleOCR training format

**Usage**:
```bash
# Generate training data
python thai-letters/quick_phase1_generator.py 10
```

**Status**: ‚úÖ **COMPLETED** - Generated full training dataset

#### `scripts/ml/deploy_sagemaker_training.py` - **TRAINING DEPLOYMENT**
**Purpose**: Deploy and execute training on AWS SageMaker

**Description**: 
- Successfully completed 25+ hour training on ml.g4dn.xlarge
- Generated working model files (9.2MB best_accuracy.pdparams)
- Handles Docker build, ECR push, and SageMaker job creation
- Monitors training progress and downloads results

**Usage**:
```bash
# Deploy training to SageMaker
python scripts/ml/deploy_sagemaker_training.py
```

**Status**: ‚úÖ **COMPLETED** - Model training successful

## Script Categories

### üîß **Infrastructure Management**
Scripts for managing AWS resources and infrastructure deployment.

### ü§ñ **Machine Learning**
Scripts for training, inference, and model management.

### üìä **Testing & Validation**
Scripts for testing permissions, validating setups, model testing, and monitoring.

#### `scripts/testing/simple_dataset_test.py`
**Purpose**: Analyze and validate Thai OCR dataset structure and quality

**Description**: 
- Performs comprehensive dataset analysis including image quality assessment
- Validates ground truth labels and character distribution
- Provides statistical overview of dataset composition
- Checks image readability and preprocessing compatibility

**Usage**:
```bash
# Run dataset analysis
python scripts/testing/simple_dataset_test.py
```

**When to use**:
- Before starting model training to validate dataset quality
- When debugging data loading issues
- To understand character distribution in training data
- For dataset quality assessment reports

**Key Features**:
- ‚úÖ Image quality analysis (brightness, contrast, dimensions)
- ‚úÖ Character frequency analysis
- ‚úÖ Ground truth validation
- ‚úÖ Statistical dataset overview

#### `scripts/testing/direct_model_test.py`
**Purpose**: Direct model testing with visual analysis and preprocessing validation

**Description**: 
- Tests trained Thai OCR models using direct Paddle inference
- Provides comprehensive visual analysis of predictions vs ground truth
- Validates model loading and configuration compatibility
- Generates detailed accuracy reports and character-level analysis

**Usage**:
```bash
# Test trained model with comprehensive analysis
python scripts/testing/direct_model_test.py

# Test with specific configuration
python scripts/testing/direct_model_test.py --config configs/rec/thai_rec_trained.yml
```

**When to use**:
- After model training completion to validate performance
- When debugging model inference issues
- For comprehensive accuracy assessment
- Before deploying models to production

**Key Features**:
- ‚úÖ Direct model inference testing
- ‚úÖ Visual prediction analysis
- ‚úÖ Character-level accuracy measurement
- ‚úÖ Configuration validation
- ‚úÖ Preprocessing pipeline testing

#### `PaddleOCR/tools/infer_rec.py` (Recommended Method)
**Purpose**: Official PaddleOCR inference tool for recognition models

**Description**: 
- Direct command-line interface for running trained recognition models
- Supports custom configurations and model paths
- Provides confidence scores and detailed logging
- Most reliable method for model inference

**Usage**:
```bash
# Navigate to PaddleOCR directory first
cd PaddleOCR

# Basic inference command
python tools/infer_rec.py \
  -c "../configs/rec/thai_rec_trained.yml" \
  -o Global.pretrained_model="../models/sagemaker_trained/best_model/model" \
  Global.infer_img="path/to/image.jpg"

# Multiple images inference
python tools/infer_rec.py \
  -c "../configs/rec/thai_rec_trained.yml" \
  -o Global.pretrained_model="../models/sagemaker_trained/best_model/model" \
  Global.infer_img="path/to/images/"
```

**When to use**:
- For production model inference (recommended method)
- When testing individual images quickly
- For batch processing multiple images
- When debugging model configuration issues

**Key Features**:
- ‚úÖ Official PaddleOCR inference interface
- ‚úÖ Confidence score reporting
- ‚úÖ Batch and single image processing
- ‚úÖ Custom configuration support
- ‚úÖ Detailed logging and error reporting

**Output Format**:
```
[2025/08/07 10:56:40] ppocr INFO: infer_img: thai_data/val/007_06.jpg
[2025/08/07 10:56:40] ppocr INFO:        result: 8  0.0988
[2025/08/07 10:56:40] ppocr INFO: success!
```

#### **VERIFIED WORKING MODEL TESTING RESULTS (August 7, 2025)**
**Training Job**: `thai-numbers-ocr-20250807-100059`
**Model Performance**:
- ‚úÖ **Inference Success**: 100% (15/15 samples)
- ‚úÖ **Model Loading**: EXCELLENT
- üìä **Character Accuracy**: 13.3% (2/15 correct predictions)
- üéØ **Successful Predictions**: Numbers '4' and '8'
- ‚ö° **Training Time**: 13 minutes
- üí∞ **Training Cost**: $0.11 USD

**Sample Results**:
```
Ground Truth ‚Üí Predicted (Confidence)
3 ‚Üí 1 (0.0958)
1 ‚Üí 3 (0.0984)  
8 ‚Üí 8 (0.0988) ‚úÖ
5 ‚Üí 3193509049 (0.0996)
4 ‚Üí 4 (0.0958) ‚úÖ
```

**Model Details**:
- **Architecture**: CRNN + MobileNetV3
- **Model Size**: 9.2MB (best_accuracy.pdparams)
- **Input Size**: 32x128 pixels
- **Character Set**: Numbers 0-9
- **Training Data**: 304 images (val: 60 images)

#### `scripts/ml/comprehensive_test.py`
**Purpose**: Comprehensive model testing framework with dictionary comparison
- Validates image preprocessing pipeline compatibility
- Performs visual analysis of model inputs without full inference
- Checks model file integrity and loads character dictionaries

**Usage**:
```bash
# Run visual model analysis
python scripts/testing/direct_model_test.py
```

**When to use**:
- After model training completion to validate model files
- Before setting up inference pipelines
- To debug preprocessing issues
- For model readiness assessment

**Key Features**:
- ‚úÖ Model file validation (.pdparams)
- ‚úÖ Image preprocessing pipeline testing
- ‚úÖ Character dictionary loading
- ‚úÖ Visual analysis reporting

#### `scripts/utils/dataset_detector.py`
**Purpose**: Automatic dataset detection and analysis for testing

**Description**: 
- Scans project for available datasets (converted, raw, output)
- Analyzes dataset structure and recommends best options for testing
- Provides detailed dataset statistics and file organization
- Identifies label files and image folders automatically

**Usage**:
```bash
# Detect and analyze datasets
python scripts/utils/dataset_detector.py
```

**When to use**:
- When setting up testing environments
- To find the best dataset for model evaluation
- For project structure exploration
- Before running comprehensive model tests

**Key Features**:
- ‚úÖ Automatic dataset discovery
- ‚úÖ Structure analysis and scoring
- ‚úÖ Label file detection
- ‚úÖ Dataset recommendation system

#### `scripts/ml/test_model_with_ground_truth.py`
**Purpose**: Comprehensive model testing with ground truth validation

**Description**: 
- Tests trained Thai OCR models against validation datasets with known answers
- Performs character-level accuracy analysis
- Generates detailed performance reports and recommendations
- Compares predicted text with ground truth labels

**Usage**:
```bash
# Run comprehensive model testing
python scripts/ml/test_model_with_ground_truth.py
```

**When to use**:
- After successful model training completion
- For production readiness assessment
- To measure character-level recognition accuracy
- When evaluating model performance improvements

**Key Features**:
- ‚úÖ Ground truth comparison testing
- ‚úÖ Character-level accuracy analysis
- ‚úÖ Performance reporting with recommendations
- ‚úÖ Sample-based testing with configurable size

---

## üìã **Quick Reference - ‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô**

### **Model Files (‡∏´‡∏•‡∏±‡∏á‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à)**
```
models/sagemaker_trained/
‚îú‚îÄ‚îÄ best_accuracy.pdparams     # ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏•‡∏±‡∏Å (9.2MB)
‚îú‚îÄ‚îÄ best_accuracy.pdopt        # Optimizer state
‚îú‚îÄ‚îÄ config.yml                 # Model configuration
‚îî‚îÄ‚îÄ best_model/
    ‚îú‚îÄ‚îÄ model.pdparams         # Alternative format
    ‚îî‚îÄ‚îÄ model.pdopt
```

### **Dictionary Files (Character Sets)**
```
thai-letters/
‚îú‚îÄ‚îÄ th_dict.txt               # ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÑ‡∏ó‡∏¢ (880 characters)
‚îú‚îÄ‚îÄ numbers_dict.txt          # ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç 0-9 (10 characters)
‚îî‚îÄ‚îÄ number_dict.txt           # Alternative numbers
```

### **Configuration Files**
```
configs/rec/
‚îú‚îÄ‚îÄ thai_rec_dev.yml          # Development (10 epochs)
‚îú‚îÄ‚îÄ thai_rec.yml              # Standard (100 epochs)
‚îú‚îÄ‚îÄ thai_rec_prod.yml         # Production (200 epochs)
‚îú‚îÄ‚îÄ test_inference_config.yml # Testing configuration
‚îú‚îÄ‚îÄ numbers_inference_config.yml # Numbers model config
‚îî‚îÄ‚îÄ quick_single_char_config.yml # Single character
```

### **Training Data Structure**
```
thai-letters/datasets/converted/train_data_thai_paddleocr_*/
‚îú‚îÄ‚îÄ train_data/rec/
‚îÇ   ‚îú‚îÄ‚îÄ rec_gt_train.txt      # Training labels
‚îÇ   ‚îú‚îÄ‚îÄ rec_gt_val.txt        # Validation labels (15 samples)
‚îÇ   ‚îî‚îÄ‚îÄ thai_data/
‚îÇ       ‚îú‚îÄ‚îÄ train/            # Training images
‚îÇ       ‚îî‚îÄ‚îÄ val/              # Validation images (15 images)
```

### **Key Scripts ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô**
```
# Data Generation
thai-letters/thai_dataset_quick.py        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

# Training
scripts/continue_deployment_v2.py         # ‡πÄ‡∏ó‡∏£‡∏ô‡∏ö‡∏ô SageMaker (‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)
scripts/training/manual_numbers_training.py # ‡πÄ‡∏ó‡∏£‡∏ô numbers (manual)
PaddleOCR/tools/train.py                  # ‡πÄ‡∏ó‡∏£‡∏ô local

# Testing
test_numbers_model.py                     # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö numbers model
test_sagemaker_model.py                   # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Thai model
PaddleOCR/tools/infer_rec.py             # Inference ‡πÅ‡∏ö‡∏ö manual
```

### **Output Files (‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå)**
```
# Test Results
numbers_model_test_results_*.json        # ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå numbers model
model_analysis_report.json               # ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•
validation_data_report.txt               # ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• validation

# Inference Results  
inference_results.txt                    # ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå inference ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
numbers_inference_results.txt            # ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå numbers inference

# Training Outputs
output/rec/                               # Local training outputs
models/model.tar.gz                      # Downloaded SageMaker model
```

### **Commands ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏£‡πá‡∏ß (Quick Training)**
```bash
# ‡πÄ‡∏ó‡∏£‡∏ô Numbers Model (13 ‡∏ô‡∏≤‡∏ó‡∏µ, $0.11)
python scripts/training/manual_numbers_training.py

# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
python test_numbers_model.py

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
cd thai-letters && python thai_dataset_quick.py 50
```

---

## Script Reference

### Infrastructure Management Scripts

#### `scripts/infrastructure/aws_manager.py`
**Purpose**: Comprehensive AWS resource management for Thai OCR project

**Description**: 
- Creates and manages S3 buckets, ECR repositories, and IAM roles
- Follows the `paddleocr-*` naming convention required by permissions
- Provides setup for complete project infrastructure

**Usage**:
```bash
# Run infrastructure setup
python scripts/infrastructure/aws_manager.py

# Or import as module
from scripts.infrastructure.aws_manager import ThaiOCRAWSManager
manager = ThaiOCRAWSManager()
resources = manager.setup_project_infrastructure("paddleocr-dev")
```

**When to use**:
- First-time project setup
- Creating new environments (dev, staging, prod)
- When you need to create AWS resources programmatically
- Before running SageMaker training jobs

**Key Features**:
- ‚úÖ S3 bucket creation with versioning
- ‚úÖ ECR repository setup with scanning
- ‚úÖ IAM role creation for SageMaker
- ‚úÖ Automatic permission validation
- ‚úÖ Resource naming following security policies

---

#### `scripts/infrastructure/deploy.sh`
**Purpose**: Complete deployment automation script

**Description**:
- End-to-end deployment pipeline for Thai OCR project
- Interactive deployment with confirmation prompts
- Handles Docker image building and pushing to ECR
- Integrates with Terraform for infrastructure deployment

**Usage**:
```bash
# Make executable (first time only)
chmod +x scripts/infrastructure/deploy.sh

# Run deployment
./scripts/infrastructure/deploy.sh

# Or run specific components interactively
./scripts/infrastructure/deploy.sh  # Follow prompts for selective deployment
```

**When to use**:
- Complete project deployment
- Setting up new environments
- Deploying after major code changes
- When you need Docker image updates

**Interactive Options**:
- ‚úÖ Infrastructure setup via Python scripts
- ‚úÖ Optional Terraform deployment
- ‚úÖ Docker image build and push
- ‚úÖ Sample data structure creation

---

### Training Configuration Scripts

#### `scripts/training/setup_training_config.py`
**Purpose**: Setup and configure PaddleOCR training configurations for Thai OCR

**Description**: 
- Automatically discovers latest converted dataset
- Creates multiple training configurations (dev, main, prod)
- Updates dictionary paths and data directories
- Adjusts hyperparameters for different training scenarios

**Usage**:
```bash
# Run Task 3: Configuration setup
python scripts/training/setup_training_config.py

# Output: configs/rec/ directory with 3 config files
# - thai_rec_dev.yml: 10 epochs for quick testing
# - thai_rec.yml: 100 epochs for main training
# - thai_rec_prod.yml: 200 epochs for production
```

**When to use**:
- After completing data preparation (Task 2)
- Before starting local or SageMaker training (Task 4)
- When you need different training configurations
- Setting up new training environments

**Key Features**:
- ‚úÖ Automatic dataset discovery and path configuration
- ‚úÖ Character dictionary path updates (th_dict.txt)
- ‚úÖ Multiple hyperparameter configurations
- ‚úÖ Data validation and verification
- ‚úÖ Comprehensive completion report generation

---

### Machine Learning Scripts

#### `scripts/ml/sagemaker_trainer.py`
**Purpose**: SageMaker training job management

**Description**:
- Creates and manages SageMaker training jobs
- Monitors training progress and logs
- Handles model creation and registration
- Designed to work with limited AWS permissions

**Usage**:
```bash
# Run training job
python scripts/ml/sagemaker_trainer.py

# Or use as module
from scripts.ml.sagemaker_trainer import ThaiOCRSageMakerTrainer
trainer = ThaiOCRSageMakerTrainer()

# Create training job
success = trainer.create_training_job(
    job_name="paddleocr-thai-training-20250730",
    role_arn="arn:aws:iam::484468818942:role/paddleocr-dev-sagemaker-role",
    image_uri="484468818942.dkr.ecr.ap-southeast-1.amazonaws.com/paddleocr-dev:latest",
    s3_input_path="s3://paddleocr-dev-data/training/",
    s3_output_path="s3://paddleocr-dev-data/models/"
)
```

**When to use**:
- Training Thai OCR models on SageMaker
- Monitoring existing training jobs
- Creating models for inference
- Scaling training beyond local resources

**Key Features**:
- ‚úÖ Training job creation and monitoring
- ‚úÖ Hyperparameter configuration
- ‚úÖ Model artifact management
- ‚úÖ CloudWatch integration for logging
- ‚úÖ Training job status tracking

---

#### `scripts/training/sagemaker_train.py`
**Purpose**: SageMaker training container entry point

**Description**:
- Main training script that runs inside SageMaker training container
- Configures PaddleOCR for CPU-only training (SageMaker optimized)
- Handles S3 data paths and training configuration
- Integrates with Docker container environment

**Usage**:
```bash
# Inside SageMaker container (automated)
python /opt/ml/code/scripts/training/sagemaker_train.py \
    --train /opt/ml/input/data/training \
    --epochs 50

# Local testing (with proper paths)
python scripts/training/sagemaker_train.py \
    --train ./train_data_thai_paddleocr_v1 \
    --epochs 10
```

**When to use**:
- Automatically executed by SageMaker training jobs
- Testing training configuration locally
- Debugging training container issues
- Custom training parameter adjustment

**Key Features**:
- ‚úÖ Automatic CPU/GPU configuration for SageMaker
- ‚úÖ S3 data path handling and validation
- ‚úÖ PaddleOCR configuration management
- ‚úÖ Training and validation dataset setup
- ‚úÖ Model output and artifact handling

---

#### `scripts/continue_deployment_v2.py`
**Purpose**: Comprehensive Docker build and SageMaker deployment automation

**Description**:
- Automated Docker image building with dependency resolution
- Handles ECR authentication and image pushing
- Creates and manages SageMaker training jobs
- Includes comprehensive error handling and status monitoring

**Usage**:
```bash
# Run complete deployment pipeline
python scripts/continue_deployment_v2.py

# The script automatically:
# 1. Builds Docker image with latest requirements
# 2. Pushes to ECR repository
# 3. Creates SageMaker training job
# 4. Monitors training progress
```

**When to use**:
- Complete deployment after code or dependency changes
- Automated CI/CD pipeline integration
- When Docker dependencies need updating
- Deploying new training configurations

**Key Features**:
- ‚úÖ Automatic Docker image building and caching
- ‚úÖ ECR authentication and image management
- ‚úÖ SageMaker job creation with proper configuration
- ‚úÖ Real-time training progress monitoring
- ‚úÖ Comprehensive error handling and logging
- ‚úÖ S3 data path validation and setup

---

### Testing & Validation Scripts

#### `scripts/testing/test_aws_permissions.py`
**Purpose**: AWS permissions validation and testing

**Description**:
- Validates all required AWS permissions are working
- Tests connectivity to S3, IAM, ECR, and SageMaker
- Generates detailed permission report
- Essential for troubleshooting access issues

**Usage**:
```bash
# Run permission tests
python scripts/testing/test_aws_permissions.py

# View results
cat aws_permissions_test_results.json
```

**When to use**:
- After initial AWS setup
- When experiencing permission errors
- Before starting training jobs
- During environment troubleshooting
- Regular health checks

**Output**:
- ‚úÖ JSON report: `aws_permissions_test_results.json`
- ‚úÖ Console logs with detailed status
- ‚úÖ Service-by-service validation results
- ‚úÖ Resource inventory (buckets, roles, repositories)

---

## Script Dependencies

### Required Python Packages
All scripts require these packages (installed via project setup):
```
boto3>=1.39.16
sagemaker>=2.248.2
scikit-image>=0.19.0
lmdb>=1.4.0
imgaug>=0.4.0
albumentations>=1.3.0
scipy>=1.9.0
matplotlib>=3.6.0
rapidfuzz>=2.0.0
```

### Docker Dependencies
Container requires these system packages:
```
libgl1-mesa-glx        # OpenGL support for OpenCV
libglib2.0-0           # GLib library
libsm6 libxext6        # X11 libraries
libxrender-dev         # X11 rendering
libgomp1              # OpenMP support
```

### AWS Configuration
Scripts require properly configured AWS credentials:
```bash
aws configure
# or use environment variables
export AWS_ACCESS_KEY_ID=your_key
export AWS_SECRET_ACCESS_KEY=your_secret
export AWS_DEFAULT_REGION=ap-southeast-1
```

### Permission Requirements
Scripts are designed to work with the permissions defined in `required_permissions.json`:
- S3: `paddleocr-*` buckets
- IAM: `paddleocr-*` roles and policies  
- ECR: `paddleocr-*` repositories
- SageMaker: Full access for training and inference
- CloudWatch: Logging access

---

## Common Usage Patterns

### üöÄ **‡πÄ‡∏ó‡∏£‡∏ô Thai OCR Model (Training Workflow)**

#### **‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å (First Time Training)**
```bash
# 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö permissions
python scripts/testing/test_aws_permissions.py

# 2. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ infrastructure
python scripts/infrastructure/aws_manager.py

# 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ó‡∏£‡∏ô
cd thai-letters
python thai_dataset_quick.py 200  # ‡∏™‡∏£‡πâ‡∏≤‡∏á 200 ‡∏†‡∏≤‡∏û
# ‚Üí ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å dictionary (th_dict.txt ‡∏´‡∏£‡∏∑‡∏≠ numbers_dict.txt)
# ‚Üí ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å effects (0=‡πÑ‡∏°‡πà‡∏°‡∏µ, 9=‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)

# 4. ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
python phase1_paddleocr_converter.py --input-path <folder> --output-path <output>

# 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á config
cd ..
python scripts/training/setup_training_config.py

# 6. ‡πÄ‡∏ó‡∏£‡∏ô‡∏ö‡∏ô SageMaker
python scripts/continue_deployment_v2.py
```

#### **‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (Numbers Training)**
```bash
# 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
cd thai-letters
python thai_dataset_quick.py 50
# ‚Üí ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å numbers_dict.txt
# ‚Üí ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å effects 9 (‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)

# 2. ‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡πá‡∏ß
python ../scripts/training/manual_numbers_training.py
# Duration: ~13 ‡∏ô‡∏≤‡∏ó‡∏µ, Cost: ~$0.11

# 3. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
python ../test_numbers_model.py
```

#### **‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏ö‡∏ö Local (Development)**
```bash
# 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å
cd thai-letters
python thai_dataset_quick.py 10  # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≠‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö

# 2. ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
python phase1_paddleocr_converter.py --input-path <folder> --output-path <output>

# 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á config ‡πÅ‡∏ö‡∏ö dev
cd ..
python scripts/training/setup_training_config.py

# 4. ‡πÄ‡∏ó‡∏£‡∏ô local
cd PaddleOCR
python tools/train.py -c ../configs/rec/thai_rec_dev.yml  # 10 epochs
```

#### **‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• (Model Testing)**
```bash
# 1. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• numbers
python test_numbers_model.py
# Output: JSON ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå + config files

# 2. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö manual
cd PaddleOCR
python tools/infer_rec.py \
  -c "../numbers_inference_config.yml" \
  -o Global.infer_img="../validation_samples/image.jpg"

# 3. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö comprehensive
python test_sagemaker_model.py
```

### Initial Project Setup
```bash
# 1. Test permissions
python scripts/testing/test_aws_permissions.py

# 2. Setup infrastructure
python scripts/infrastructure/aws_manager.py

# 3. Full deployment (optional)
./scripts/infrastructure/deploy.sh
```

### Training Workflow
```bash
# 1. Prepare data (see thai-letters/ scripts)
python thai-letters/quick_phase1_generator.py
python thai-letters/phase1_paddleocr_converter.py

# 2. Setup training configurations
python scripts/training/setup_training_config.py

# 3. Test locally (optional)
python PaddleOCR/tools/train.py -c configs/rec/thai_rec_dev.yml

# 4. Deploy to SageMaker
python scripts/continue_deployment_v2.py

# 5. Monitor training progress
aws logs tail /aws/sagemaker/TrainingJobs --follow
```

### Docker Development Workflow
```bash
# 1. Update dependencies
# Edit requirements.txt with new packages

# 2. Test Docker build locally
docker build -f Dockerfile.sagemaker -t test-thai-ocr .

# 3. Clear cache if needed
docker system prune -af

# 4. Deploy with updated container
python scripts/continue_deployment_v2.py
```

### Troubleshooting
```bash
# Check permissions
python scripts/testing/test_aws_permissions.py

# Check AWS CLI config
aws configure list
aws sts get-caller-identity
```

---

## Error Handling

### Common Issues and Solutions

### Troubleshooting

#### **‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Debug ‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤**

**1. ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Model ‡πÅ‡∏•‡∏∞ Configuration**:
```bash
# ‡∏î‡∏π model files
ls -la models/sagemaker_trained/
# ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ: best_accuracy.pdparams, config.yml

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö dictionary
wc -l thai-letters/th_dict.txt
# Output: 880 lines (characters)

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö training data
head -5 thai-letters/datasets/converted/*/train_data/rec/rec_gt_val.txt
# Format: image_path\tground_truth
```

**2. ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Configuration Files**:
```bash
# ‡∏î‡∏π config ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö inference
cat test_inference_config.yml
cat numbers_inference_config.yml

# ‡∏î‡∏π training config
cat configs/rec/thai_rec.yml
```

**3. ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢**:
```bash
# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö single image
cd PaddleOCR
python tools/infer_rec.py \
  -c "../test_inference_config.yml" \
  -o Global.infer_img="../validation_samples/001.jpg"
```

**4. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Log Files**:
```bash
# ‡∏î‡∏π training logs (local)
tail -50 output/rec/train.log

# ‡∏î‡∏π inference results
cat inference_results.txt
cat numbers_inference_results.txt
```

#### **Common Issues and Solutions**

**‡∏õ‡∏±‡∏ç‡∏´‡∏≤: ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ**:
```bash
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•
ls -la models/sagemaker_trained/best_accuracy.*
# ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ: .pdparams ‡πÅ‡∏•‡∏∞ .pdopt

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö config
python -c "import yaml; print(yaml.safe_load(open('models/sagemaker_trained/config.yml')))"
```

**‡∏õ‡∏±‡∏ç‡∏´‡∏≤: Dictionary ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô**:
```bash
# ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö dictionary
wc -l thai-letters/th_dict.txt      # 880 lines
wc -l thai-letters/numbers_dict.txt # 10 lines

# ‡πÉ‡∏ä‡πâ dictionary ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô
# Numbers model ‚Üí ‡πÉ‡∏ä‡πâ numbers_dict.txt
# Thai model ‚Üí ‡πÉ‡∏ä‡πâ th_dict.txt
```

**‡∏õ‡∏±‡∏ç‡∏´‡∏≤: ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô**:
```bash
# ‡πÉ‡∏ä‡πâ dev config ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö (10 epochs)
python tools/train.py -c ../configs/rec/thai_rec_dev.yml

# ‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
python thai-letters/thai_dataset_quick.py 10  # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏Ñ‡πà 10 ‡∏†‡∏≤‡∏û
```

**‡∏õ‡∏±‡∏ç‡∏´‡∏≤: AWS Permissions**:
```bash
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö permissions
python scripts/testing/test_aws_permissions.py

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö AWS credentials
aws sts get-caller-identity
aws configure list
```

#### **Files ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Debug**:
- `inference_results.txt` - ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö inference
- `numbers_inference_results.txt` - ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå numbers model
- `validation_data_report.txt` - ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• validation
- `model_analysis_report.json` - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•
- `numbers_model_test_results_*.json` - ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö

**Permission Denied Errors**:
- Run `python test_aws_permissions.py` to validate access
- Check that resource names follow `paddleocr-*` pattern
- Verify AWS credentials are current

**Training Job Failures**:
- Check CloudWatch logs via AWS Console
- Verify S3 paths and data format
- Ensure Docker image is properly built and pushed

**Docker Build Issues**:
- **ModuleNotFoundError for dependencies**: Update `requirements.txt` with missing packages
- **libGL.so.1 missing**: Add `libgl1-mesa-glx` to Dockerfile system packages
- **Container build failures**: Clear Docker cache with `docker system prune -af`
- **Dependency conflicts**: Check package versions in requirements.txt

**PaddleOCR Training Issues**:
- **Distributed training errors**: Disable GPU and distributed training for SageMaker
- **Data path errors**: Ensure S3 paths point to correct directory structure
- **rapidfuzz missing**: Add `rapidfuzz>=2.0.0` to requirements.txt
- **scikit-image missing**: Add `scikit-image>=0.19.0` to requirements.txt

**S3 Data Structure Issues**:
- **FileNotFoundError for labels**: Check that `rec_gt_train.txt` and `rec_gt_val.txt` exist
- **Image path errors**: Verify thai_data/train/ contains training images
- **Path configuration**: Update data_dir in training config to include '/rec/' suffix

**Resource Already Exists**:
- Scripts handle existing resources gracefully
- Use unique suffixes for resource names when needed

**ECR Authentication Issues**:
- Run `aws ecr get-login-password` to refresh Docker login
- Verify ECR repository exists and has proper permissions
- Check AWS region configuration matches ECR region

---

## Script Maintenance

### Adding New Scripts
When creating new scripts:

1. **Follow naming conventions**:
   - Use descriptive names: `feature_action.py` or `action_feature.sh`
   - Place in appropriate `scripts/` subdirectory if needed

2. **Update documentation**:
   - Add entry to this file (`doc/scripts.md`)
   - Include purpose, usage, and when to use
   - Update other relevant documentation

3. **Follow code standards**:
   - Include docstrings and comments
   - Add error handling and logging
   - Test with limited permissions

4. **Update related files**:
   - Add to `.gitignore` if generates temporary files
   - Update `README.md` if script is user-facing
   - Update `development-task.md` if part of workflow

### Script Organization

```
scripts/
‚îú‚îÄ‚îÄ infrastructure/             # AWS resource management
‚îÇ   ‚îú‚îÄ‚îÄ aws_manager.py         # Main AWS resource manager
‚îÇ   ‚îî‚îÄ‚îÄ deploy.sh              # Complete deployment automation
‚îú‚îÄ‚îÄ ml/                        # Machine learning operations
‚îÇ   ‚îî‚îÄ‚îÄ sagemaker_trainer.py   # SageMaker training jobs
‚îú‚îÄ‚îÄ testing/                   # Testing and validation
‚îÇ   ‚îî‚îÄ‚îÄ test_aws_permissions.py # AWS permissions validation
‚îî‚îÄ‚îÄ utils/                     # Utility scripts (future)
    ‚îú‚îÄ‚îÄ data_helpers.py        # Data processing utilities
    ‚îî‚îÄ‚îÄ monitoring.py          # Monitoring and alerting
```

---

## Integration with Other Components

### With Terraform
- Scripts complement Terraform for dynamic resource creation
- Use scripts for resources that need runtime configuration
- Terraform handles static infrastructure

### With thai-letters/
- Data preparation scripts in `thai-letters/` generate training data
- Scripts in `scripts/` handle cloud deployment and training
- Clear separation between data generation and cloud operations

### With Documentation
- All scripts must be documented in this file
- Usage examples should be practical and tested
- Keep documentation updated when scripts change

---

## Future Enhancements

### Planned Scripts
- `scripts/ml/inference_endpoint.py`: SageMaker endpoint management
- `scripts/ml/model_deployment.py`: Model versioning and deployment
- `scripts/utils/monitoring.py`: Training and inference monitoring
- `scripts/utils/data_pipeline.py`: Automated data preparation pipeline
- `scripts/infrastructure/cost_optimizer.py`: AWS cost optimization tools

### Improvement Areas
- Enhanced error handling and retry logic
- Configuration file support (YAML/JSON)
- Integration with CI/CD pipelines
- Automated testing for scripts
- Cross-platform compatibility enhancements

---

## üöÄ **SageMaker Training Scripts**

### `scripts/continue_deployment_v2.py` - **COMPLETE SAGEMAKER TRAINING AUTOMATION**
**Purpose**: Fully automated Thai OCR training deployment on AWS SageMaker

**Description**: 
- Complete end-to-end SageMaker training automation
- Docker container building and ECR push
- Training job creation and monitoring
- Handles all AWS resource management automatically
- Verified working configuration for Thai OCR training

**Usage**:
```bash
# Complete automated deployment (recommended)
python scripts/continue_deployment_v2.py

# The script will:
# 1. Build Docker container with PaddleOCR and dependencies
# 2. Push container to ECR repository
# 3. Upload training data to S3 (if needed)
# 4. Create SageMaker training job
# 5. Monitor training progress
# 6. Download trained model artifacts
```

**When to use**:
- Starting a new training run on SageMaker
- Need complete automated setup
- Production training deployments

**Key Features**:
- ‚úÖ Automated Docker build and ECR push
- ‚úÖ S3 data upload management
- ‚úÖ SageMaker job creation with optimal settings
- ‚úÖ Real-time training monitoring
- ‚úÖ Cost tracking and estimation
- ‚úÖ Error handling and retry logic

### `scripts/deploy_sagemaker_training.py` - **MANUAL SAGEMAKER TRAINING**
**Purpose**: Manual SageMaker training job creation and management

**Description**: 
- Create SageMaker training jobs with custom parameters
- Support for different instance types and configurations
- Manual control over training hyperparameters
- Integration with existing Docker containers

**Usage**:
```bash
# Basic training job
python scripts/deploy_sagemaker_training.py

# With custom parameters
python scripts/deploy_sagemaker_training.py \
  --instance-type ml.g4dn.xlarge \
  --epochs 100 \
  --learning-rate 0.001
```

**When to use**:
- Need custom training configurations
- Experimenting with different hyperparameters
- Manual control over training process

**Key Features**:
- ‚úÖ Customizable training parameters
- ‚úÖ Multiple instance type support
- ‚úÖ Integration with existing infrastructure
- ‚úÖ Manual training job management

### `scripts/easy_single_char_training.py` - **SIMPLIFIED TRAINING SETUP**
**Purpose**: Simplified setup for single character Thai OCR training

**Description**: 
- Streamlined setup for single character recognition
- Optimized configurations for Thai character training
- Reduced complexity for quick experiments
- Focus on single character accuracy improvement

**Usage**:
```bash
# Quick single character training setup
python scripts/easy_single_char_training.py

# With specific configuration
python scripts/easy_single_char_training.py --config quick_single_char_config.yml
```

**When to use**:
- Focusing on single character recognition
- Quick experiments and testing
- Debugging character-specific issues

**Key Features**:
- ‚úÖ Single character optimizations
- ‚úÖ Simplified configuration management
- ‚úÖ Quick setup and testing
- ‚úÖ Thai character specific enhancements

### `scripts/training/sagemaker_train.py` - **SAGEMAKER TRAINING ENTRY POINT**
**Purpose**: Main training script executed inside SageMaker containers

**Description**: 
- Entry point script for SageMaker training jobs
- Handles SageMaker environment setup
- Integrates with PaddleOCR training pipeline
- Manages model output and artifacts

**Usage**:
```bash
# This script is executed automatically by SageMaker
# Inside the Docker container during training
python sagemaker_train.py --epochs 100 --learning_rate 0.001
```

**When to use**:
- Executed automatically by SageMaker
- Part of the Docker container training process
- No direct user interaction required

**Key Features**:
- ‚úÖ SageMaker environment integration
- ‚úÖ PaddleOCR training pipeline integration
- ‚úÖ Model artifact management
- ‚úÖ Training monitoring and logging

### `scripts/utils/monitor_training.py` - **TRAINING MONITORING UTILITY**
**Purpose**: Real-time monitoring of SageMaker training jobs

**Description**: 
- Real-time training job status monitoring
- CloudWatch logs integration
- Training metrics visualization
- Cost tracking during training

**Usage**:
```bash
# Monitor specific training job
python scripts/utils/monitor_training.py paddleocr-thai-training-20250807-120000

# Continuous monitoring with custom interval
python scripts/utils/monitor_training.py <job-name> --interval 60
```

**When to use**:
- Monitoring active training jobs
- Tracking training progress
- Debugging training issues

**Key Features**:
- ‚úÖ Real-time status updates
- ‚úÖ CloudWatch logs integration
- ‚úÖ Training metrics display
- ‚úÖ Cost estimation and tracking