#!/usr/bin/env python3
"""
ğŸ§ª Test SageMaker Trained Thai OCR Model
à¸—à¸”à¸ªà¸­à¸šà¹‚à¸¡à¹€à¸”à¸¥ Thai OCR à¸—à¸µà¹ˆà¹€à¸—à¸£à¸™à¸ˆà¸²à¸ SageMaker

Uses:
- Model: models/sagemaker_trained/best_model/
- Config: CRNN + MobileNetV3 (same as training)
- Dictionary: thai-letters/th_dict.txt (880 chars)
- Validation data: with ground truth labels
"""

import os
import sys
import subprocess
import json
import time
from pathlib import Path
from typing import List, Dict, Tuple
import logging

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SageMakerModelTester:
    """à¸—à¸”à¸ªà¸­à¸šà¹‚à¸¡à¹€à¸”à¸¥à¸—à¸µà¹ˆà¹€à¸—à¸£à¸™à¸ˆà¸²à¸ SageMaker"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent
        self.paddleocr_dir = self.project_root / "PaddleOCR"
        self.model_dir = self.project_root / "models" / "sagemaker_trained" / "best_model"
        self.config_file = self.project_root / "corrected_inference_config.yml"
        self.dict_file = self.project_root / "thai-letters" / "th_dict.txt"
        self.val_label_file = self.project_root / "thai-letters" / "datasets" / "converted" / "train_data_thai_paddleocr_0804_1144" / "train_data" / "rec" / "rec_gt_val.txt"
        self.results = []
        
    def check_prerequisites(self) -> bool:
        """à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¹„à¸Ÿà¸¥à¹Œà¸—à¸µà¹ˆà¸ˆà¸³à¹€à¸›à¹‡à¸™"""
        logger.info("ğŸ” Checking prerequisites...")
        
        checks = [
            (self.paddleocr_dir, "PaddleOCR directory"),
            (self.model_dir / "model.pdparams", "Model parameters"),
            (self.dict_file, "Thai dictionary"),
            (self.val_label_file, "Validation labels"),
        ]
        
        all_good = True
        for path, description in checks:
            if path.exists():
                size = path.stat().st_size if path.is_file() else "Directory"
                logger.info(f"âœ… {description}: {path} ({size} bytes)" if size != "Directory" else f"âœ… {description}: {path}")
            else:
                logger.error(f"âŒ {description}: {path} NOT FOUND")
                all_good = False
        
        # Check if inference config exists, if not create one
        if not self.config_file.exists():
            logger.warning(f"âš ï¸ Config file not found: {self.config_file}")
            logger.info("ğŸ“ Will create inference config automatically")
        
        return all_good
    
    def load_validation_data(self, max_samples: int = 20) -> List[Tuple[str, str]]:
        """à¹‚à¸«à¸¥à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ validation à¸à¸£à¹‰à¸­à¸¡ ground truth"""
        logger.info(f"ğŸ“ Loading validation data (max {max_samples} samples)...")
        
        validation_data = []
        
        try:
            with open(self.val_label_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()[:max_samples]
                
            for line in lines:
                line = line.strip()
                if '\t' in line:
                    # Format: image_path\tground_truth_text
                    parts = line.split('\t', 1)
                    if len(parts) == 2:
                        rel_image_path, ground_truth = parts
                        
                        # Convert relative path to absolute
                        # rel_image_path format: thai_data/val/346_01.jpg
                        full_image_path = self.project_root / "thai-letters" / "datasets" / "converted" / "train_data_thai_paddleocr_0804_1144" / "train_data" / "rec" / rel_image_path
                        
                        if full_image_path.exists():
                            validation_data.append((str(full_image_path), ground_truth))
                        else:
                            logger.warning(f"Image not found: {full_image_path}")
                            
            logger.info(f"âœ… Loaded {len(validation_data)} validation samples")
            
            # Show first few samples
            for i, (img_path, gt) in enumerate(validation_data[:3]):
                logger.info(f"  Sample {i+1}: {Path(img_path).name} -> '{gt}'")
            
            return validation_data
            
        except Exception as e:
            logger.error(f"Failed to load validation data: {e}")
            return []
    
    def create_inference_config(self) -> str:
        """à¸ªà¸£à¹‰à¸²à¸‡à¸„à¸­à¸™à¸Ÿà¸´à¸à¸ªà¸³à¸«à¸£à¸±à¸š inference"""
        logger.info("âš™ï¸ Creating inference configuration...")
        
        # Use corrected config instead of creating new one
        if self.config_file.exists():
            logger.info(f"âœ… Using corrected config: {self.config_file}")
            return str(self.config_file)
        
        # Fallback: create config only if corrected one doesn't exist
        config_content = f"""Global:
  use_gpu: false
  epoch_num: 100
  log_smooth_window: 20
  print_batch_step: 10
  save_model_dir: ./output/thai_rec/
  save_epoch_step: 10
  eval_batch_step: 500
  cal_metric_during_train: true
  pretrained_model: {self.model_dir}/model
  character_dict_path: {self.dict_file}
  character_type: thai
  max_text_length: 25
  infer_mode: true
  use_space_char: false
  distributed: false
  save_res_path: ./inference_results.txt
  infer_img: ./test_images/

Optimizer:
  name: Adam
  beta1: 0.9
  beta2: 0.999
  lr:
    name: Cosine
    learning_rate: 0.001
    warmup_epoch: 5
  regularizer:
    name: L2
    factor: 3.0e-05

Architecture:
  model_type: rec
  algorithm: CRNN
  Transform:
  Backbone:
    name: MobileNetV3
    scale: 0.5
    model_name: large
    disable_se: false
  Neck:
    name: SequenceEncoder
    encoder_type: rnn
    hidden_size: 96
  Head:
    name: CTCHead
    fc_decay: 0.00001

Loss:
  name: CTCLoss

PostProcess:
  name: CTCLabelDecode

Metric:
  name: RecMetric
  main_indicator: acc

Train:
  dataset:
    name: SimpleDataSet
    data_dir: ./train_data/rec/
    label_file_list:
    - ./train_data/rec/rec_gt_train.txt
    transforms:
    - DecodeImage:
        img_mode: BGR
        channel_first: false
    - CTCLabelEncode:
    - RecResizeImg:
        image_shape: [3, 32, 100]
    - KeepKeys:
        keep_keys:
        - image
        - label
        - length
  loader:
    shuffle: true
    batch_size_per_card: 256
    drop_last: true
    num_workers: 8

Eval:
  dataset:
    name: SimpleDataSet
    data_dir: ./train_data/rec/
    label_file_list:
    - ./train_data/rec/rec_gt_val.txt
    transforms:
    - DecodeImage:
        img_mode: BGR
        channel_first: false
    - CTCLabelEncode:
    - RecResizeImg:
        image_shape: [3, 32, 100]
    - KeepKeys:
        keep_keys:
        - image
        - label
        - length
  loader:
    shuffle: false
    drop_last: false
    batch_size_per_card: 256
    num_workers: 4
"""
        
        # Save config
        test_config_file = self.project_root / "test_inference_config.yml"
        with open(test_config_file, 'w', encoding='utf-8') as f:
            f.write(config_content)
            
        logger.info(f"âœ… Config created: {test_config_file}")
        return str(test_config_file)
    
    def run_inference_on_image(self, image_path: str, config_file: str) -> Dict:
        """à¸£à¸±à¸™ inference à¸šà¸™à¸ à¸²à¸à¹€à¸”à¸µà¸¢à¸§"""
        
        try:
            # Build command - use relative paths from PaddleOCR directory
            rel_config = os.path.relpath(config_file, self.paddleocr_dir)
            rel_image = os.path.relpath(image_path, self.paddleocr_dir)
            
            cmd = [
                "python", "tools/infer_rec.py",
                "-c", rel_config,
                "-o", f"Global.infer_img={rel_image}"
            ]
            
            logger.debug(f"Running: {' '.join(cmd)} (cwd: {self.paddleocr_dir})")
            
            # Run inference
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                cwd=str(self.paddleocr_dir),
                timeout=30
            )
            
            # Parse result
            if result.returncode == 0 and "result:" in result.stdout:
                # Extract prediction from output
                for line in result.stdout.split('\n'):
                    if "result:" in line:
                        # Format: "result: predicted_text confidence_score"
                        result_part = line.split("result:")[1].strip()
                        
                        # Try to split text and confidence
                        parts = result_part.rsplit(' ', 1)
                        if len(parts) == 2:
                            predicted_text, confidence_str = parts
                            try:
                                confidence = float(confidence_str)
                            except ValueError:
                                predicted_text = result_part
                                confidence = 0.0
                        else:
                            predicted_text = result_part
                            confidence = 0.0
                        
                        # à¸ªà¸³à¸«à¸£à¸±à¸š single character à¹€à¸­à¸²à¹à¸„à¹ˆà¸•à¸±à¸§à¹à¸£à¸
                        if predicted_text:
                            single_char = predicted_text[0] if predicted_text else ""
                            
                            return {
                                "success": True,
                                "predicted_text": single_char,  # à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹€à¸›à¹‡à¸™ single char
                                "full_prediction": predicted_text,  # à¹€à¸à¹‡à¸š full prediction à¹„à¸§à¹‰à¸”à¹‰à¸§à¸¢
                                "confidence": confidence,
                                "raw_output": result.stdout
                            }
            
            return {
                "success": False,
                "error": result.stderr if result.stderr else "No result found in output",
                "raw_output": result.stdout,
                "returncode": result.returncode
            }
            
        except subprocess.TimeoutExpired:
            return {
                "success": False,
                "error": "Inference timeout (30s)",
                "raw_output": ""
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "raw_output": ""
            }
    
    def calculate_accuracy(self, predicted: str, ground_truth: str) -> Dict:
        """à¸„à¸³à¸™à¸§à¸“à¸„à¸§à¸²à¸¡à¹à¸¡à¹ˆà¸™à¸¢à¸³"""
        
        # Character-level accuracy
        char_correct = 0
        max_len = max(len(predicted), len(ground_truth))
        
        if max_len == 0:
            char_accuracy = 1.0 if predicted == ground_truth else 0.0
        else:
            for i in range(min(len(predicted), len(ground_truth))):
                if i < len(predicted) and i < len(ground_truth):
                    if predicted[i] == ground_truth[i]:
                        char_correct += 1
            char_accuracy = char_correct / max_len
        
        # Exact match
        exact_match = predicted == ground_truth
        
        # Edit distance (simple)
        edit_distance = abs(len(predicted) - len(ground_truth))
        for i in range(min(len(predicted), len(ground_truth))):
            if predicted[i] != ground_truth[i]:
                edit_distance += 1
        
        return {
            "character_accuracy": char_accuracy,
            "exact_match": exact_match,
            "edit_distance": edit_distance,
            "predicted_length": len(predicted),
            "ground_truth_length": len(ground_truth)
        }
    
    def run_batch_test(self, max_samples: int = 10) -> Dict:
        """à¸£à¸±à¸™à¸à¸²à¸£à¸—à¸”à¸ªà¸­à¸šà¹à¸šà¸š batch"""
        logger.info(f"ğŸ§ª Starting batch testing (max {max_samples} samples)...")
        
        # Load validation data
        validation_data = self.load_validation_data(max_samples)
        if not validation_data:
            return {"error": "No validation data loaded"}
        
        # Create inference config
        config_file = self.create_inference_config()
        
        # Test each sample
        results = []
        successful_inferences = 0
        total_character_accuracy = 0.0
        exact_matches = 0
        
        print(f"\n{'='*80}")
        print(f"ğŸ§ª TESTING {len(validation_data)} SAMPLES")
        print(f"{'='*80}")
        
        for i, (image_path, ground_truth) in enumerate(validation_data):
            print(f"\n[{i+1:2d}/{len(validation_data)}] Testing: {Path(image_path).name}")
            print(f"   Ground Truth: '{ground_truth}'")
            
            # Run inference
            inference_result = self.run_inference_on_image(image_path, config_file)
            
            # Calculate accuracy if successful
            if inference_result["success"]:
                successful_inferences += 1
                predicted_text = inference_result["predicted_text"]
                
                accuracy_metrics = self.calculate_accuracy(predicted_text, ground_truth)
                
                total_character_accuracy += accuracy_metrics["character_accuracy"]
                if accuracy_metrics["exact_match"]:
                    exact_matches += 1
                
                # Store result
                result = {
                    "image": Path(image_path).name,
                    "ground_truth": ground_truth,
                    "predicted": predicted_text,
                    "confidence": inference_result["confidence"],
                    "metrics": accuracy_metrics,
                    "success": True
                }
                
                # Status icon
                status_icon = "âœ…" if accuracy_metrics["exact_match"] else "âš ï¸"
                print(f"   {status_icon} Predicted: '{predicted_text}'")
                print(f"   ğŸ“Š Confidence: {inference_result['confidence']:.4f} | Char Acc: {accuracy_metrics['character_accuracy']:.1%}")
                
            else:
                result = {
                    "image": Path(image_path).name,
                    "ground_truth": ground_truth,
                    "predicted": "",
                    "confidence": 0.0,
                    "error": inference_result["error"],
                    "success": False
                }
                
                print(f"   âŒ ERROR: {inference_result['error'][:60]}...")
                if inference_result.get('raw_output'):
                    print(f"      Raw output: {inference_result['raw_output'][:100]}...")
            
            results.append(result)
        
        # Calculate overall metrics
        avg_character_accuracy = total_character_accuracy / max(successful_inferences, 1)
        exact_match_rate = exact_matches / len(validation_data)
        success_rate = successful_inferences / len(validation_data)
        
        summary = {
            "total_samples": len(validation_data),
            "successful_inferences": successful_inferences,
            "success_rate": success_rate,
            "average_character_accuracy": avg_character_accuracy,
            "exact_match_rate": exact_match_rate,
            "exact_matches": exact_matches
        }
        
        return {
            "summary": summary,
            "detailed_results": results,
            "config_used": config_file
        }
    
    def print_summary_report(self, test_results: Dict):
        """à¹à¸ªà¸”à¸‡à¸£à¸²à¸¢à¸‡à¸²à¸™à¸ªà¸£à¸¸à¸›"""
        print("\n" + "="*80)
        print("ğŸ¯ SAGEMAKER THAI OCR MODEL TEST RESULTS")
        print("="*80)
        
        if "error" in test_results:
            print(f"âŒ Test failed: {test_results['error']}")
            return
        
        summary = test_results["summary"]
        
        print(f"ğŸ“Š OVERALL PERFORMANCE:")
        print(f"  â€¢ Total samples tested: {summary['total_samples']}")
        print(f"  â€¢ Successful inferences: {summary['successful_inferences']}")
        print(f"  â€¢ Success rate: {summary['success_rate']:.1%}")
        print(f"  â€¢ Average character accuracy: {summary['average_character_accuracy']:.1%}")
        print(f"  â€¢ Exact match rate: {summary['exact_match_rate']:.1%}")
        print(f"  â€¢ Exact matches: {summary['exact_matches']}/{summary['total_samples']}")
        
        # Performance assessment
        print(f"\nğŸ¯ PERFORMANCE ASSESSMENT:")
        if summary['success_rate'] >= 0.8:
            print("  âœ… Model loading: GOOD")
        else:
            print("  âŒ Model loading: ISSUES DETECTED")
            
        if summary['average_character_accuracy'] >= 0.7:
            print("  âœ… Character accuracy: EXCELLENT")
        elif summary['average_character_accuracy'] >= 0.5:
            print("  âš ï¸ Character accuracy: MODERATE")
        else:
            print("  âŒ Character accuracy: NEEDS IMPROVEMENT")
            
        if summary['exact_match_rate'] >= 0.5:
            print("  âœ… Exact matches: EXCELLENT")
        elif summary['exact_match_rate'] >= 0.3:
            print("  âš ï¸ Exact matches: MODERATE")
        else:
            print("  âŒ Exact matches: NEEDS IMPROVEMENT")
        
        print(f"\nğŸ“ SAMPLE PREDICTIONS:")
        successful_results = [r for r in test_results["detailed_results"] if r["success"]]
        
        for i, result in enumerate(successful_results[:8]):  # Show first 8
            status = "âœ…" if result["metrics"]["exact_match"] else "âš ï¸"
            print(f"  {status} {result['image']}")
            print(f"     GT: '{result['ground_truth']}'")
            print(f"     Predicted: '{result['predicted']}'")
            print(f"     Conf: {result['confidence']:.4f} | Acc: {result['metrics']['character_accuracy']:.1%}")
        
        if len(successful_results) > 8:
            print(f"  ... and {len(successful_results) - 8} more successful predictions")
        
        failed_results = [r for r in test_results["detailed_results"] if not r["success"]]
        if failed_results:
            print(f"\nâŒ FAILED PREDICTIONS:")
            for result in failed_results[:3]:
                print(f"  â€¢ {result['image']}: {result['error'][:50]}...")
        
        print(f"\nâš™ï¸ MODEL CONFIGURATION:")
        print(f"  â€¢ Model: models/sagemaker_trained/best_model/")
        print(f"  â€¢ Architecture: CRNN + MobileNetV3")
        print(f"  â€¢ Dictionary: thai-letters/th_dict.txt")
        print(f"  â€¢ Max text length: 1")
        print(f"  â€¢ Config: {Path(test_results['config_used']).name}")
        
        print(f"\nğŸ’¡ RECOMMENDATIONS:")
        if summary['average_character_accuracy'] < 0.5:
            print("  âš ï¸ Consider improving accuracy:")
            print("     - Retrain with optimized 74-char dictionary")
            print("     - Adjust preprocessing parameters")
            print("     - Use data augmentation")
        
        if summary['success_rate'] < 0.8:
            print("  âš ï¸ Fix inference issues:")
            print("     - Check PaddleOCR version compatibility")
            print("     - Verify model file integrity")
            print("     - Update configuration settings")
        
        if summary['exact_match_rate'] > 0.3:
            print("  âœ… Model shows promising results!")
            print("     - Consider deploying to SageMaker endpoint")
            print("     - Test with real-world images")
    
    def save_results(self, test_results: Dict, filename: str = None):
        """à¸šà¸±à¸™à¸—à¸¶à¸à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ"""
        if filename is None:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            filename = f"sagemaker_model_test_results_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(test_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“„ Results saved to: {filename}")
        return filename

def main():
    """Main function"""
    print("ğŸš€ SageMaker Thai OCR Model Tester")
    print("=" * 60)
    
    tester = SageMakerModelTester()
    
    # Check prerequisites
    if not tester.check_prerequisites():
        print("âŒ Prerequisites check failed. Please ensure all required files exist.")
        return
    
    print("\nğŸ§ª Starting model testing...")
    
    # Run batch test
    test_results = tester.run_batch_test(max_samples=15)
    
    # Print summary
    tester.print_summary_report(test_results)
    
    # Save results
    if "error" not in test_results:
        result_file = tester.save_results(test_results)
        print(f"\nğŸ“„ Detailed results saved to: {result_file}")
    
    print(f"\nâœ… Testing completed!")
    print("=" * 60)

if __name__ == "__main__":
    main()
